{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"TPU","kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":85992,"sourceType":"modelInstanceVersion","modelInstanceId":72251,"modelId":76277}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üß† Gemma2 Reasoning GRPO - Full Production\n\n**Google Tunix Hackathon 2026**\n\n### Core Architecture:\n- **Hardware:** TPU v5e-8 (2 Data x 4 Tensor Mesh)\n- **Model:** Gemma 2 2B-IT (CPU-Offloaded bf16 Init)\n- **Fine-Tuning:** LoRA on Attention + MLP layers\n- **Strategy:** GRPO with 16 Parallel Generations per prompt\n- **Rewards:** Format (25%) + Logic (30%) + Accuracy (45%) + Self-Correction & Length Regularization","metadata":{}},{"cell_type":"markdown","source":"## üì¶ Cell 1: Environment Setup\n*Run once, then restart kernel*","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\nimport glob\nimport time\n\n# =============================================================================\n# FORCE RESET: Smart Cleanup of ALL old markers\n# =============================================================================\n# This defines the NEW marker we want to create after success\nCURRENT_MARKER = \"/kaggle/working/.setup_complete_ff\"\n\nprint(\"üßπ Cleaning up old installation markers...\")\n\n# Use glob to find ANY file starting with \".setup_complete\"\nold_markers = glob.glob(\"/kaggle/working/.setup_complete*\")\n\nfor marker in old_markers:\n    # Don't delete the current one if it essentially already exists (though we usually want to overwrite)\n    try:\n        os.remove(marker)\n        print(f\"   üóëÔ∏è Deleted old marker: {marker}\")\n    except OSError as e:\n        print(f\"   ‚ö†Ô∏è Could not delete {marker}: {e}\")\n\n# =============================================================================\n# INSTALLATION (Aggressive Cleanup Mode)\n# =============================================================================\nprint(\"=\"*60)\nprint(\"üîÑ STARTING AGGRESSIVE CLEAN INSTALLATION...\")\nprint(\"=\"*60)\n\n# 1. Update pip\n%pip install --upgrade pip -q\n\n# 2. AGGRESSIVELY UNINSTALL CONFLICTING PACKAGES\n# We must remove these to prevent the \"PyExtensionType\" error\nprint(\"üóëÔ∏è Uninstalling conflicting libraries...\")\n%pip uninstall -y datasets pyarrow pandas huggingface_hub -q 2>/dev/null\n\n# 3. Install JAX for TPU\nprint(\"‚¨áÔ∏è Installing JAX/TPU stack...\")\n%pip install -q \"jax[tpu]>=0.8.0\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n\n# 4. Install Tunix & Qwix\nprint(\"‚¨áÔ∏è Installing Tunix & Qwix...\")\n%pip install -q git+https://github.com/google/tunix\n%pip install -q git+https://github.com/google/qwix\n%pip uninstall -q flax -y 2>/dev/null\n%pip install -q git+https://github.com/google/flax\n\n# 5. CRITICAL FIX: Install Compatible Data Libraries\n# We force specific versions that are known to work together\nprint(\"‚¨áÔ∏è Installing Data Libraries (The Fix)...\")\n%pip install -q \"numpy==2.0.0\" \"pyarrow==17.0.0\" \"datasets>=2.21.0\" \"pandas>=2.2.0\"\n%pip install -q kagglehub transformers grain huggingface_hub tensorflow tensorflow_datasets\n\n# 6. Create the NEW marker\nwith open(CURRENT_MARKER, \"w\") as f: \n    f.write(\"done\")\n    \nprint(\"\\n\" + \"=\"*60)\nprint(\"‚úÖ INSTALLATION COMPLETE.\")\nprint(\"‚ö†Ô∏è PLEASE RESTART KERNEL NOW (‚ü≥ Button)!\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T04:56:18.836124Z","iopub.execute_input":"2026-01-21T04:56:18.836371Z","iopub.status.idle":"2026-01-21T04:56:52.919495Z","shell.execute_reply.started":"2026-01-21T04:56:18.836349Z","shell.execute_reply":"2026-01-21T04:56:52.918542Z"}},"outputs":[{"name":"stdout","text":"üßπ Cleaning up old installation markers...\n   üóëÔ∏è Deleted old marker: /kaggle/working/.setup_complete_ff\n============================================================\nüîÑ STARTING AGGRESSIVE CLEAN INSTALLATION...\n============================================================\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\nüóëÔ∏è Uninstalling conflicting libraries...\nNote: you may need to restart the kernel to use updated packages.\n‚¨áÔ∏è Installing JAX/TPU stack...\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n‚¨áÔ∏è Installing Tunix & Qwix...\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n‚¨áÔ∏è Installing Data Libraries (The Fix)...\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-metrax 0.2.4 requires numpy>=2.1.3, but you have numpy 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n\n============================================================\n‚úÖ INSTALLATION COMPLETE.\n‚ö†Ô∏è PLEASE RESTART KERNEL NOW (‚ü≥ Button)!\n============================================================\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## üîå Cell 2: Imports & Memory Monitor","metadata":{}},{"cell_type":"code","source":"import os, re, gc, time, shutil\nfrom pathlib import Path\nfrom typing import List, Dict, Any\n\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport optax\nimport grain\nimport qwix\nimport kagglehub\n\nfrom flax import nnx\nfrom orbax import checkpoint as ocp\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\n\n# Tunix imports\ntry:\n    from tunix.models.gemma2 import model as gemma_model_lib\n    from tunix.models.gemma2 import params as gemma_params_lib\n    print(\"‚úì Using tunix.models.gemma2\")\nexcept ImportError:\n    from tunix.models.gemma import model as gemma_model_lib\n    from tunix.models.gemma import params as gemma_params_lib\n    print(\"‚úì Using tunix.models.gemma (fallback)\")\n\nfrom tunix.rl import rl_cluster as rl_cluster_lib\nfrom tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\nfrom tunix.rl.rollout import base_rollout\nfrom tunix.generate import sampler as sampler_lib\n\nclass MemoryMonitor:\n    @staticmethod\n    def get_usage():\n        try:\n            stats = [d.memory_stats() for d in jax.devices() if d.memory_stats()]\n            if stats:\n                used = sum(s['bytes_in_use'] for s in stats)\n                limit = sum(s['bytes_limit'] for s in stats)\n                return used, limit\n        except:\n            pass\n        return 0, 0\n    \n    @staticmethod\n    def print_summary():\n        used, limit = MemoryMonitor.get_usage()\n        if limit > 0:\n            print(f\"  TPU Memory: {used/1e9:.2f}GB / {limit/1e9:.2f}GB ({100*used/limit:.1f}%)\")\n        else:\n            print(\"  TPU Memory: stats unavailable\")\n    \n    @staticmethod\n    def check_available(required_gb=20):\n        used, limit = MemoryMonitor.get_usage()\n        available = (limit - used) / 1e9\n        return available >= required_gb\n\nmonitor = MemoryMonitor()\nprint(f\"JAX: {jax.__version__} | TPU Cores: {len(jax.devices())}\")\nmonitor.print_summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T04:56:52.920281Z","iopub.execute_input":"2026-01-21T04:56:52.920446Z","iopub.status.idle":"2026-01-21T04:57:11.372278Z","shell.execute_reply.started":"2026-01-21T04:56:52.920427Z","shell.execute_reply":"2026-01-21T04:57:11.370816Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/site-packages/jax/_src/cloud_tpu_init.py:93: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n  warnings.warn(\n/usr/local/lib/python3.12/site-packages/torch_xla/__init__.py:258: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"‚úì Using tunix.models.gemma (fallback)\n","output_type":"stream"},{"name":"stderr","text":"WARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1768971420.405157    4258 common_lib.cc:650] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: ===\nlearning/45eac/tfrc/runtime/common_lib.cc:238\n","output_type":"stream"},{"name":"stdout","text":"JAX: 0.8.2 | TPU Cores: 8\n  TPU Memory: 0.00GB / 135.27GB (0.0%)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## ‚öôÔ∏è Cell 3: Production Configuration\n*16 Generations | 600 Steps | Optimized for TPU v5e-8*","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# MODEL CONFIGURATION\n# =============================================================================\nMODEL_VERSION = \"gemma2-2b-it\"\nMODEL_PATH = \"google/gemma-2/flax/gemma2-2b-it\"\nMODEL_HF_NAME = \"google/gemma-2-2b-it\"\n\n# =============================================================================\n# MESH CONFIGURATION\n# =============================================================================\nMESH_SHAPE = (2, 4)\nMESH_AXES = (\"fsdp\", \"tp\")\n\n# =============================================================================\n# LORA CONFIGURATION\n# =============================================================================\nLORA_RANK = 64\nLORA_ALPHA = 64.0\nLORA_TARGET_PATTERN = \".*q_einsum|.*kv_einsum|.*o_proj|.*gate_proj|.*up_proj|.*down_proj\"\n\n# =============================================================================\n# GRPO CONFIGURATION (SAFETY MODE)\n# =============================================================================\nNUM_GENERATIONS = 8     # Keep at 8\nNUM_ITERATIONS = 2      # Reduced to 2 for speed\nBETA = 0.04             \nEPSILON = 0.2           \n\n# =============================================================================\n# TRAINING CONFIGURATION (SAFETY MODE)\n# =============================================================================\nMAX_STEPS = 600\nLEARNING_RATE = 2e-6\nWARMUP_STEPS = 40\nWEIGHT_DECAY = 0.01\n\n# CRITICAL CHANGE: Reduced to 1 to prevent OOM\nMINI_BATCH_SIZE = 1    \nMICRO_BATCH_SIZE = 1    \n\n# =============================================================================\n# SEQUENCE CONFIGURATION (SAFETY MODE)\n# =============================================================================\nMAX_PROMPT_LENGTH = 256\n# CRITICAL CHANGE: Reduced to 300 (Saves ~2GB VRAM)\nMAX_GENERATION_LENGTH = 300\n\n# =============================================================================\n# REWARD WEIGHTS\n# =============================================================================\nREWARD_WEIGHT_FORMAT = 0.25      \nREWARD_WEIGHT_LOGIC = 0.30       \nREWARD_WEIGHT_ACCURACY = 0.45    \n\n# =============================================================================\n# OUTPUT TAGS\n# =============================================================================\nTAG_REASONING_START = \"<reasoning>\"\nTAG_REASONING_END = \"</reasoning>\"\nTAG_ANSWER_START = \"<answer>\"\nTAG_ANSWER_END = \"</answer>\"\n\n# =============================================================================\n# PATHS\n# =============================================================================\nCKPT_DIR = \"/kaggle/working/checkpoints\"\nOUTPUT_DIR = \"/kaggle/working/gemma2-grpo-safe\"\n\n# =============================================================================\n# DATA CONFIGURATION\n# =============================================================================\nNUM_TRAIN_SAMPLES = 448\nNUM_TEST_SAMPLES = 64 \nRANDOM_SEED = 42\n\nimport os\nos.makedirs(CKPT_DIR, exist_ok=True)\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(\"=\"*60)\nprint(\"‚úÖ SAFETY CONFIGURATION LOADED\")\nprint(\"=\"*60)\nprint(f\"   Batch Size: {MINI_BATCH_SIZE} (Minimum)\")\nprint(f\"   Gen Length: {MAX_GENERATION_LENGTH} (Safe)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T04:57:11.373217Z","iopub.execute_input":"2026-01-21T04:57:11.373813Z","iopub.status.idle":"2026-01-21T04:57:11.381341Z","shell.execute_reply.started":"2026-01-21T04:57:11.373790Z","shell.execute_reply":"2026-01-21T04:57:11.380328Z"}},"outputs":[{"name":"stdout","text":"============================================================\n‚úÖ SAFETY CONFIGURATION LOADED\n============================================================\n   Batch Size: 1 (Minimum)\n   Gen Length: 300 (Safe)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## üéØ Cell 4: Weighted Reward Functions\n*Format (25%) + Logic (30%) + Accuracy (45%) + Bonuses*","metadata":{}},{"cell_type":"code","source":"def format_reward(prompts, completions, **kwargs):\n    \"\"\"\n    Reward for correct XML tag structure (25% weight).\n    Checks: <reasoning>...</reasoning> followed by <answer>...</answer>\n    \"\"\"\n    rewards = []\n    for c in completions:\n        score = 0.0\n        has_r_start = TAG_REASONING_START in c\n        has_r_end = TAG_REASONING_END in c\n        has_a_start = TAG_ANSWER_START in c\n        has_a_end = TAG_ANSWER_END in c\n        \n        # Full format with correct ordering\n        if has_r_start and has_r_end and has_a_start and has_a_end:\n            r_end_pos = c.find(TAG_REASONING_END)\n            a_start_pos = c.find(TAG_ANSWER_START)\n            if r_end_pos < a_start_pos:\n                score = 1.0  # Perfect format\n            else:\n                score = 0.5  # Wrong order\n        # Partial format\n        elif (has_r_start and has_r_end) or (has_a_start and has_a_end):\n            score = 0.3\n        \n        rewards.append(score * REWARD_WEIGHT_FORMAT)\n    return rewards\n\n\ndef logic_reward(prompts, completions, **kwargs):\n    \"\"\"\n    Reward for quality reasoning with logical transitions (30% weight).\n    Checks: step-by-step reasoning, transition words, mathematical operations.\n    \"\"\"\n    TRANSITIONS = [\n        'therefore', 'because', 'since', 'so', 'thus', 'hence',\n        'first', 'second', 'third', 'then', 'next', 'finally',\n        'step', 'calculate', 'compute', 'multiply', 'divide', 'add', 'subtract'\n    ]\n    \n    rewards = []\n    for c in completions:\n        score = 0.0\n        \n        # Extract reasoning section\n        match = re.search(f'{TAG_REASONING_START}(.*?){TAG_REASONING_END}', c, re.DOTALL | re.IGNORECASE)\n        if match:\n            content = match.group(1).lower()\n            \n            # Count transition words (up to 0.4)\n            trans_count = sum(1 for t in TRANSITIONS if t in content)\n            score += min(0.4, trans_count * 0.05)\n            \n            # Count explicit steps like \"Step 1:\", \"Step 2:\" (up to 0.3)\n            step_count = len(re.findall(r'step\\s*\\d', content))\n            score += min(0.3, step_count * 0.1)\n            \n            # Check for mathematical expressions (up to 0.2)\n            math_ops = len(re.findall(r'\\d+\\s*[+\\-√ó√∑*/]\\s*\\d+', content))\n            score += min(0.2, math_ops * 0.05)\n            \n            # Bonus for \"equals\" or \"=\" showing computation (up to 0.1)\n            equals_count = content.count('=') + content.count('equals')\n            score += min(0.1, equals_count * 0.02)\n        \n        rewards.append(min(1.0, score) * REWARD_WEIGHT_LOGIC)\n    return rewards\n\n\ndef accuracy_reward(prompts, completions, **kwargs):\n    \"\"\"\n    Reward for correct numerical answer (45% weight).\n    Compares extracted answer with ground truth.\n    \"\"\"\n    answers = kwargs.get('answers', [])\n    rewards = []\n    \n    for i, c in enumerate(completions):\n        score = 0.0\n        \n        if i >= len(answers):\n            rewards.append(0.0)\n            continue\n        \n        # Extract answer from completion\n        match = re.search(f'{TAG_ANSWER_START}(.*?){TAG_ANSWER_END}', c, re.DOTALL)\n        if not match:\n            rewards.append(0.0)\n            continue\n        \n        pred_text = match.group(1).strip()\n        truth_text = str(answers[i]).strip()\n        \n        # Extract numbers\n        pred_nums = re.findall(r'-?\\d+\\.?\\d*', pred_text)\n        truth_nums = re.findall(r'-?\\d+\\.?\\d*', truth_text)\n        \n        if pred_nums and truth_nums:\n            try:\n                pred = float(pred_nums[-1])  # Use last number as final answer\n                truth = float(truth_nums[-1])\n                \n                if pred == truth:\n                    score = 1.0  # Exact match\n                elif abs(truth) > 0.001:\n                    ratio = pred / truth\n                    if 0.99 <= ratio <= 1.01:\n                        score = 0.9  # Within 1%\n                    elif 0.9 <= ratio <= 1.1:\n                        score = 0.5  # Within 10%\n            except (ValueError, ZeroDivisionError):\n                pass\n        \n        rewards.append(score * REWARD_WEIGHT_ACCURACY)\n    return rewards\n\n\ndef self_correction_reward(prompts, completions, **kwargs):\n    \"\"\"\n    Bonus reward for self-correction behavior (additive, max 0.1).\n    Encourages the model to catch and fix its own mistakes.\n    \"\"\"\n    CORRECTION_PHRASES = [\n        \"wait\", \"actually\", \"let me recalculate\", \"i made an error\",\n        \"correction\", \"re-checking\", \"that's not right\", \"let me redo\"\n    ]\n    \n    rewards = []\n    for c in completions:\n        c_lower = c.lower()\n        count = sum(1 for phrase in CORRECTION_PHRASES if phrase in c_lower)\n        # Cap at 0.1 bonus\n        rewards.append(min(0.1, count * 0.03))\n    return rewards\n\n\ndef length_reward(prompts, completions, **kwargs):\n    \"\"\"\n    Length regularization (additive, max 0.05).\n    Rewards appropriate response length, penalizes too short/long.\n    \"\"\"\n    rewards = []\n    for c in completions:\n        words = len(c.split())\n        if 100 <= words <= 400:\n            rewards.append(0.05)  # Ideal length\n        elif 50 <= words <= 600:\n            rewards.append(0.02)  # Acceptable\n        else:\n            rewards.append(0.0)   # Too short or too long\n    return rewards\n\n\n# Combine all reward functions\nREWARD_FUNCTIONS = [\n    format_reward,      # 25% weight\n    logic_reward,       # 30% weight  \n    accuracy_reward,    # 45% weight\n    self_correction_reward,  # Bonus up to 10%\n    length_reward,      # Bonus up to 5%\n]\n\nprint(\"‚úÖ Reward Functions Defined\")\nprint(f\"   ‚Ä¢ format_reward: {REWARD_WEIGHT_FORMAT*100}% weight\")\nprint(f\"   ‚Ä¢ logic_reward: {REWARD_WEIGHT_LOGIC*100}% weight\")\nprint(f\"   ‚Ä¢ accuracy_reward: {REWARD_WEIGHT_ACCURACY*100}% weight\")\nprint(f\"   ‚Ä¢ self_correction_reward: bonus (max 10%)\")\nprint(f\"   ‚Ä¢ length_reward: bonus (max 5%)\")\nprint(f\"   Total potential: {(REWARD_WEIGHT_FORMAT + REWARD_WEIGHT_LOGIC + REWARD_WEIGHT_ACCURACY)*100}% + 15% bonus\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T04:57:11.382242Z","iopub.execute_input":"2026-01-21T04:57:11.382438Z","iopub.status.idle":"2026-01-21T04:57:11.396158Z","shell.execute_reply.started":"2026-01-21T04:57:11.382420Z","shell.execute_reply":"2026-01-21T04:57:11.395130Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Reward Functions Defined\n   ‚Ä¢ format_reward: 25.0% weight\n   ‚Ä¢ logic_reward: 30.0% weight\n   ‚Ä¢ accuracy_reward: 45.0% weight\n   ‚Ä¢ self_correction_reward: bonus (max 10%)\n   ‚Ä¢ length_reward: bonus (max 5%)\n   Total potential: 100.0% + 15% bonus\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## üìä Cell 5: Data Loading (GSM8K)","metadata":{}},{"cell_type":"code","source":"# System prompt teaching the expected output format\nSYSTEM_PROMPT = f\"\"\"You are a mathematical problem solver who shows their work.\n\nFor each problem:\n1. Think through it step-by-step inside {TAG_REASONING_START} and {TAG_REASONING_END} tags\n2. Show your calculations clearly\n3. Give your final numerical answer inside {TAG_ANSWER_START} and {TAG_ANSWER_END} tags\n\nExample:\n{TAG_REASONING_START}\nStep 1: Identify what we need to find.\nStep 2: Set up the calculation.\nStep 3: 5 √ó 10 = 50\nTherefore, the answer is 50.\n{TAG_REASONING_END}\n{TAG_ANSWER_START}\n50\n{TAG_ANSWER_END}\"\"\"\n\ndef format_prompt(question: str) -> str:\n    \"\"\"Format a question with the system prompt.\"\"\"\n    return f\"{SYSTEM_PROMPT}\\n\\nQuestion: {question}\\nSolution:\"\n\ndef extract_answer(answer_text: str) -> str:\n    \"\"\"Extract the final answer from GSM8K format (after ####).\"\"\"\n    if '####' in answer_text:\n        return answer_text.split('####')[-1].strip()\n    return answer_text.strip()\n\n# Load GSM8K dataset\nprint(\"Loading GSM8K dataset...\")\n# FIX: Use \"gsm8k\" instead of \"openai/gsm8k\" to prevent path resolution errors\ndataset = load_dataset(\"gsm8k\", \"main\", split=\"train\")\ndataset = dataset.shuffle(seed=RANDOM_SEED)\n\n# Prepare data\nall_data = []\nfor i, item in enumerate(dataset):\n    # Fetch enough data to cover both train and test requests\n    if i >= NUM_TRAIN_SAMPLES + NUM_TEST_SAMPLES + 16: # Buffer for filtering\n        break\n    all_data.append({\n        'prompt': format_prompt(item['question']),\n        'answer': extract_answer(item['answer'])\n    })\n\n# Ensure sizes are multiples of 8 (Required for TPU mesh distribution)\n# We calculate the largest multiple of 8 that fits within your requested sample size\ntrain_size = (min(len(all_data), NUM_TRAIN_SAMPLES) // 8) * 8\nremaining_after_train = len(all_data) - train_size\ntest_size = (min(remaining_after_train, NUM_TEST_SAMPLES) // 8) * 8\n\ntrain_data = all_data[:train_size]\ntest_data = all_data[train_size : train_size + test_size]\n\n# Verify batch alignment\nfull_batch_train = len(train_data) * NUM_GENERATIONS\nfull_batch_test = len(test_data) * NUM_GENERATIONS\n\nprint(f\"‚úÖ Data Loaded\")\nprint(f\"   Train: {len(train_data)} samples √ó {NUM_GENERATIONS} gen = {full_batch_train} (√∑{MINI_BATCH_SIZE}={full_batch_train//MINI_BATCH_SIZE})\")\nprint(f\"   Test: {len(test_data)} samples √ó {NUM_GENERATIONS} gen = {full_batch_test} (√∑{MINI_BATCH_SIZE}={full_batch_test//MINI_BATCH_SIZE})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T04:57:11.396835Z","iopub.execute_input":"2026-01-21T04:57:11.397033Z","iopub.status.idle":"2026-01-21T04:57:12.769310Z","shell.execute_reply.started":"2026-01-21T04:57:11.397014Z","shell.execute_reply":"2026-01-21T04:57:12.767987Z"}},"outputs":[{"name":"stdout","text":"Loading GSM8K dataset...\n‚úÖ Data Loaded\n   Train: 448 samples √ó 8 gen = 3584 (√∑1=3584)\n   Test: 64 samples √ó 8 gen = 512 (√∑1=512)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## üîß Cell 6: Mesh & Tokenizer","metadata":{}},{"cell_type":"code","source":"# Create mesh for distributed training\nmesh = jax.make_mesh(\n    MESH_SHAPE, \n    MESH_AXES, \n    axis_types=(jax.sharding.AxisType.Auto,) * len(MESH_SHAPE)\n)\nprint(f\"‚úì Mesh created: {mesh}\")\n\n# Get HuggingFace token\nhf_token = os.environ.get('HF_TOKEN')\nif not hf_token:\n    try:\n        from kaggle_secrets import UserSecretsClient\n        hf_token = UserSecretsClient().get_secret(\"HF_TOKEN\")\n        print(\"‚úì HF Token from Kaggle Secrets\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è No HF_TOKEN found: {e}\")\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_HF_NAME, token=hf_token)\n\n# Get EOS tokens for generation stopping\nEOS_TOKENS = [tokenizer.eos_token_id]\ntry:\n    end_turn_id = tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")\n    if end_turn_id != tokenizer.unk_token_id:\n        EOS_TOKENS.append(end_turn_id)\nexcept:\n    pass\n\nprint(f\"‚úÖ Tokenizer Ready | EOS tokens: {EOS_TOKENS}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T04:57:12.769870Z","iopub.execute_input":"2026-01-21T04:57:12.770062Z","iopub.status.idle":"2026-01-21T04:57:13.573263Z","shell.execute_reply.started":"2026-01-21T04:57:12.770045Z","shell.execute_reply":"2026-01-21T04:57:13.572144Z"}},"outputs":[{"name":"stdout","text":"‚úì Mesh created: Mesh('fsdp': 2, 'tp': 4, axis_types=(Auto, Auto))\n‚úì HF Token from Kaggle Secrets\n‚úÖ Tokenizer Ready | EOS tokens: [1, 107]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## ü§ñ Cell 7: CPU-Offload Model Loading + LoRA\n*Build on CPU first to prevent TPU OOM during initialization*","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# STEP 1: Clean memory\n# =============================================================================\ngc.collect()\njax.clear_caches()\nprint(\"1. Memory cleared\")\nmonitor.print_summary()\n\n# =============================================================================\n# STEP 2: Download model\n# =============================================================================\nprint(\"\\n2. Downloading model from Kaggle...\")\nk_path = kagglehub.model_download(MODEL_PATH)\nckpt_path = os.path.join(k_path, MODEL_VERSION)\nprint(f\"   Path: {ckpt_path}\")\n\n# =============================================================================\n# STEP 3: Get CPU device for offloading\n# =============================================================================\ntry:\n    cpu_device = jax.devices('cpu')[0]\n    print(f\"\\n3. CPU device for offloading: {cpu_device}\")\nexcept:\n    cpu_device = jax.devices()[0] # Fallback if no CPU (unlikely)\n    print(\"\\n3. No separate CPU device found\")\n\n# =============================================================================\n# STEP 4: Build model & Apply LoRA (FORCED ON CPU)\n# =============================================================================\n# CRITICAL FIX: We use 'with jax.default_device(cpu_device)' to ensure \n# initial random weights are allocated on RAM, not TPU VRAM.\nprint(\"\\n4. Building model and applying LoRA on CPU...\")\n\nwith jax.default_device(cpu_device):\n    # A. Build the model structure (allocates on CPU)\n    print(\"   Initializing model structure...\")\n    config = gemma_model_lib.ModelConfig.gemma2_2b_it()\n    base_model = gemma_model_lib.Gemma(config, rngs=nnx.Rngs(RANDOM_SEED))\n    \n    # B. Load weights (allocates on CPU)\n    print(\"   Loading checkpoint weights...\")\n    raw_params = gemma_params_lib.load_and_format_params(ckpt_path)\n    print(\"   Converting to bf16...\")\n    bf16_params = jax.tree.map(lambda x: x.astype(jnp.bfloat16), raw_params)\n    nnx.update(base_model, bf16_params)\n    \n    # Clean up raw params to free RAM\n    del raw_params, bf16_params\n    gc.collect()\n    \n    # C. Split into Reference and Actor\n    print(\"   Creating Actor/Reference copies...\")\n    graph, state = nnx.split(base_model)\n    ref_model = nnx.merge(graph, state) # Frozen reference\n    actor_base = nnx.merge(graph, state) # Base for actor\n    \n    # D. Apply LoRA to Actor (happens on CPU)\n    print(f\"   Applying LoRA (rank={LORA_RANK})...\")\n    lora_provider = qwix.LoraProvider(\n        module_path=LORA_TARGET_PATTERN,\n        rank=LORA_RANK,\n        alpha=LORA_ALPHA,\n    )\n    \n    # Get shape and apply\n    model_input = actor_base.get_model_input()\n    actor = qwix.apply_lora_to_model(\n        actor_base,\n        lora_provider,\n        rngs=nnx.Rngs(RANDOM_SEED),\n        **model_input\n    )\n    print(\"   ‚úì LoRA applied on CPU\")\n\n# Clean up base model artifacts\ndel base_model, actor_base\ngc.collect()\n\n# =============================================================================\n# STEP 5: Shard EVERYTHING to TPU mesh\n# =============================================================================\nprint(\"\\n5. Sharding models to TPU mesh (2√ó4)...\")\n\ndef shard_model_to_tpu(model, name):\n    print(f\"   Moving {name} to TPU...\")\n    with mesh:\n        # 1. Get current state (on CPU)\n        state = nnx.state(model)\n        # 2. Calculate where each slice belongs\n        pspecs = nnx.get_partition_spec(state)\n        # 3. Move it (This is the heavy data transfer)\n        sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n        # 4. Update the model wrapper\n        nnx.update(model, sharded_state)\n\nshard_model_to_tpu(ref_model, \"Reference Model\")\nshard_model_to_tpu(actor, \"Actor Model\")\n\nprint(\"   ‚úì All models sharded successfully\")\nmonitor.print_summary()\n\n# =============================================================================\n# FINAL STATUS\n# =============================================================================\nprint(\"\\n\" + \"=\"*60)\nprint(\"‚úÖ MODEL READY\")\nprint(\"=\"*60)\nprint(f\"   Actor: Gemma2-2B-IT + LoRA (rank={LORA_RANK})\")\nprint(f\"   Reference: Gemma2-2B-IT (frozen)\")\nmonitor.print_summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T04:57:13.574081Z","iopub.execute_input":"2026-01-21T04:57:13.574268Z","iopub.status.idle":"2026-01-21T04:58:14.565220Z","shell.execute_reply.started":"2026-01-21T04:57:13.574250Z","shell.execute_reply":"2026-01-21T04:58:14.563684Z"}},"outputs":[{"name":"stdout","text":"1. Memory cleared\n  TPU Memory: 0.00GB / 135.27GB (0.0%)\n\n2. Downloading model from Kaggle...\n   Path: /kaggle/input/gemma-2/flax/gemma2-2b-it/1/gemma2-2b-it\n\n3. CPU device for offloading: TFRT_CPU_0\n\n4. Building model and applying LoRA on CPU...\n   Initializing model structure...\n","output_type":"stream"},{"name":"stderr","text":"ERROR:asyncio:Exception in callback Task.__step()\nhandle: <Handle Task.__step()>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\nRuntimeError: cannot enter context: <_contextvars.Context object at 0x78248676dd40> is already entered\nERROR:asyncio:Exception in callback Task.__step()\nhandle: <Handle Task.__step()>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\nRuntimeError: cannot enter context: <_contextvars.Context object at 0x78248676dd40> is already entered\nERROR:asyncio:Exception in callback Task.__step()\nhandle: <Handle Task.__step()>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\nRuntimeError: cannot enter context: <_contextvars.Context object at 0x78248676dd40> is already entered\nERROR:asyncio:Exception in callback Task.__step()\nhandle: <Handle Task.__step()>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\nRuntimeError: cannot enter context: <_contextvars.Context object at 0x78248676dd40> is already entered\nERROR:asyncio:Exception in callback Task.__step()\nhandle: <Handle Task.__step()>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\nRuntimeError: cannot enter context: <_contextvars.Context object at 0x78248676dd40> is already entered\nWARNING:absl:`StandardCheckpointHandler` expects a target tree to be provided for restore. Not doing so is generally UNSAFE unless you know the present topology to be the same one as the checkpoint was saved under.\nERROR:asyncio:Exception in callback Task.__step()\nhandle: <Handle Task.__step()>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\nRuntimeError: cannot enter context: <_contextvars.Context object at 0x78248676dd40> is already entered\nERROR:asyncio:Exception in callback Task.__step()\nhandle: <Handle Task.__step()>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\nRuntimeError: cannot enter context: <_contextvars.Context object at 0x78248676dd40> is already entered\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-3' coro=<_async_in_context.<locals>.run_in_context() done, defined at /usr/local/lib/python3.12/site-packages/ipykernel/utils.py:57> wait_for=<Task pending name='Task-4' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:597> cb=[Task.__wakeup()]> cb=[ZMQStream._run_callback.<locals>._log_error() at /usr/local/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py:563]>\n/usr/local/lib/python3.12/json/decoder.py:354: RuntimeWarning: coroutine 'Kernel.shell_main' was never awaited\n  obj, end = self.scan_once(s, idx)\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-4' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:597> cb=[Task.__wakeup()]>\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-5' coro=<_async_in_context.<locals>.run_in_context() done, defined at /usr/local/lib/python3.12/site-packages/ipykernel/utils.py:57> wait_for=<Task pending name='Task-6' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:597> cb=[Task.__wakeup()]> cb=[ZMQStream._run_callback.<locals>._log_error() at /usr/local/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py:563]>\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-6' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:597> cb=[Task.__wakeup()]>\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-7' coro=<_async_in_context.<locals>.run_in_context() done, defined at /usr/local/lib/python3.12/site-packages/ipykernel/utils.py:57> wait_for=<Task pending name='Task-8' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:597> cb=[Task.__wakeup()]> cb=[ZMQStream._run_callback.<locals>._log_error() at /usr/local/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py:563]>\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-8' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:597> cb=[Task.__wakeup()]>\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-9' coro=<_async_in_context.<locals>.run_in_context() done, defined at /usr/local/lib/python3.12/site-packages/ipykernel/utils.py:57> wait_for=<Task pending name='Task-10' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:597> cb=[Task.__wakeup()]> cb=[ZMQStream._run_callback.<locals>._log_error() at /usr/local/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py:563]>\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-10' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:597> cb=[Task.__wakeup()]>\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-11' coro=<_async_in_context.<locals>.run_in_context() done, defined at /usr/local/lib/python3.12/site-packages/ipykernel/utils.py:57> wait_for=<Task pending name='Task-12' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:597> cb=[Task.__wakeup()]> cb=[ZMQStream._run_callback.<locals>._log_error() at /usr/local/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py:563]>\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-12' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:597> cb=[Task.__wakeup()]>\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-13' coro=<_async_in_context.<locals>.run_in_context() done, defined at /usr/local/lib/python3.12/site-packages/ipykernel/utils.py:57> wait_for=<Task pending name='Task-15' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:597> cb=[Task.__wakeup()]> cb=[ZMQStream._run_callback.<locals>._log_error() at /usr/local/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py:563]>\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-15' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:597> cb=[Task.__wakeup()]>\n","output_type":"stream"},{"name":"stdout","text":"   Loading checkpoint weights...\n   Converting to bf16...\n","output_type":"stream"},{"name":"stderr","text":"ERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-43' coro=<_async_in_context.<locals>.run_in_context() running at /usr/local/lib/python3.12/site-packages/ipykernel/utils.py:60> wait_for=<Task pending name='Task-2' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:597> cb=[Task.task_wakeup()]> cb=[ZMQStream._run_callback.<locals>._log_error() at /usr/local/lib/python3.12/site-packages/zmq/eventloop/zmqstream.py:563]>\n/tmp/ipykernel_4258/1852922428.py:49: RuntimeWarning: coroutine 'Kernel.shell_main' was never awaited\n  gc.collect()\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\nERROR:asyncio:Task was destroyed but it is pending!\ntask: <Task pending name='Task-2' coro=<Kernel.shell_main() running at /usr/local/lib/python3.12/site-packages/ipykernel/kernelbase.py:597> cb=[Task.task_wakeup()]>\n","output_type":"stream"},{"name":"stdout","text":"   Creating Actor/Reference copies...\n   Applying LoRA (rank=64)...\n   ‚úì LoRA applied on CPU\n\n5. Sharding models to TPU mesh (2√ó4)...\n   Moving Reference Model to TPU...\n   Moving Actor Model to TPU...\n   ‚úì All models sharded successfully\n  TPU Memory: 105.33GB / 135.27GB (77.9%)\n\n============================================================\n‚úÖ MODEL READY\n============================================================\n   Actor: Gemma2-2B-IT + LoRA (rank=64)\n   Reference: Gemma2-2B-IT (frozen)\n  TPU Memory: 105.33GB / 135.27GB (77.9%)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## üìö Cell 8: Data Loaders (Grain MapDataset)","metadata":{}},{"cell_type":"code","source":"class GSM8KDataSource:\n    \"\"\"Data source compatible with Grain MapDataset.\"\"\"\n    def __init__(self, data):\n        self._data = data\n    \n    def __len__(self):\n        return len(self._data)\n    \n    def __getitem__(self, idx):\n        item = self._data[idx]\n        # Tunix GRPO expects 'prompts' and 'answers' keys\n        return {\n            'prompts': item['prompt'],\n            'answers': item['answer']\n        }\n\n# Create Grain MapDataset pipelines\n# Using the same pattern as official Tunix GRPO demo\n# FIX: Added .batch() to bundle examples into groups of 8\ntrain_dataset = (\n    grain.MapDataset.source(GSM8KDataSource(train_data))\n    .shuffle(seed=RANDOM_SEED)\n    .batch(MINI_BATCH_SIZE, drop_remainder=True)\n)\n\nval_dataset = (\n    grain.MapDataset.source(GSM8KDataSource(test_data))\n    .batch(MINI_BATCH_SIZE, drop_remainder=True)\n)\n\nprint(\"‚úÖ Grain Datasets Created (Batched)\")\nprint(f\"   Train: {len(train_data)} prompts -> {len(train_data)//MINI_BATCH_SIZE} batches\")\nprint(f\"   Val: {len(test_data)} prompts -> {len(test_data)//MINI_BATCH_SIZE} batches\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T04:58:50.210749Z","iopub.execute_input":"2026-01-21T04:58:50.211136Z","iopub.status.idle":"2026-01-21T04:58:50.217788Z","shell.execute_reply.started":"2026-01-21T04:58:50.211117Z","shell.execute_reply":"2026-01-21T04:58:50.216530Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Grain Datasets Created (Batched)\n   Train: 448 prompts -> 448 batches\n   Val: 64 prompts -> 64 batches\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## ‚ö° Cell 9: Training Configuration","metadata":{}},{"cell_type":"code","source":"# Learning rate schedule with warmup and cosine decay\nlr_schedule = optax.warmup_cosine_decay_schedule(\n    init_value=0.0,\n    peak_value=LEARNING_RATE,\n    warmup_steps=WARMUP_STEPS,\n    decay_steps=MAX_STEPS,\n    end_value=LEARNING_RATE * 0.1\n)\n\n# Optimizer with gradient clipping\noptimizer = optax.chain(\n    optax.clip_by_global_norm(1.0),\n    optax.adamw(lr_schedule, weight_decay=WEIGHT_DECAY)\n)\n\n# Cluster configuration\ncluster_config = rl_cluster_lib.ClusterConfig(\n    role_to_mesh={\n        rl_cluster_lib.Role.ACTOR: mesh,\n        rl_cluster_lib.Role.REFERENCE: mesh,\n        rl_cluster_lib.Role.ROLLOUT: mesh,\n    },\n    training_config=rl_cluster_lib.RLTrainingConfig(\n        actor_optimizer=optimizer,\n        max_steps=MAX_STEPS,\n        eval_every_n_steps=25,\n        mini_batch_size=MINI_BATCH_SIZE,\n        train_micro_batch_size=MICRO_BATCH_SIZE,\n        checkpoint_root_directory=CKPT_DIR,\n    ),\n    rollout_config=base_rollout.RolloutConfig(\n        max_tokens_to_generate=MAX_GENERATION_LENGTH,\n        max_prompt_length=MAX_PROMPT_LENGTH,\n        temperature=0.9,\n        top_p=0.95,\n        eos_tokens=EOS_TOKENS,\n    )\n)\n\n# GRPO configuration - 16 generations!\ngrpo_config = GRPOConfig(\n    num_generations=NUM_GENERATIONS,  # G=16\n    num_iterations=NUM_ITERATIONS,    # Œº=3\n    beta=BETA,                        # Œ≤=0.04\n    epsilon=EPSILON,                  # Œµ=0.2\n)\n\nprint(\"‚úÖ Training Configuration\")\nprint(f\"   LR Schedule: warmup {WARMUP_STEPS} ‚Üí {LEARNING_RATE} ‚Üí cosine decay\")\nprint(f\"   Optimizer: AdamW (clip=1.0, decay={WEIGHT_DECAY})\")\nprint(f\"   GRPO: G={NUM_GENERATIONS}, Œº={NUM_ITERATIONS}, Œ≤={BETA}, Œµ={EPSILON}\")\nprint(f\"   Eval every {25} steps\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T04:58:54.978942Z","iopub.execute_input":"2026-01-21T04:58:54.979216Z","iopub.status.idle":"2026-01-21T04:58:54.985497Z","shell.execute_reply.started":"2026-01-21T04:58:54.979198Z","shell.execute_reply":"2026-01-21T04:58:54.984423Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Training Configuration\n   LR Schedule: warmup 40 ‚Üí 2e-06 ‚Üí cosine decay\n   Optimizer: AdamW (clip=1.0, decay=0.01)\n   GRPO: G=8, Œº=2, Œ≤=0.04, Œµ=0.2\n   Eval every 25 steps\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## üéì Cell 10: Create GRPO Trainer","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# STEP 1: Smart Imports & Component Recovery\n# =============================================================================\nimport tunix\nimport inspect\nimport gc\nimport jax\nimport optax  # Needed for the Real Optimizer Fix\nfrom tunix.rl.grpo import grpo_learner\nfrom tunix.rl import rl_cluster\nfrom tunix.rl import rl_learner\nfrom tunix.rl.rollout import base_rollout\n\nprint(\"1. Initializing Tunix components...\")\n\n# --- FIX 0: CLEANUP MEMORY ---\ngc.collect()\njax.clear_caches()\n\n# --- FIX 1: Find ClusterConfig ---\nClusterConfigClass = None\nfor name, obj in inspect.getmembers(rl_cluster):\n    if inspect.isclass(obj) and \"ClusterConfig\" in name:\n        ClusterConfigClass = obj\n        break\nif not ClusterConfigClass:\n    raise ImportError(\"CRITICAL: No ClusterConfig class found.\")\n\n# --- FIX 2: Find TrainingConfig ---\nTrainingConfigClass = None\nsearch_modules = [rl_learner, grpo_learner]\ntry:\n    import tunix.common.config as common_config\n    search_modules.append(common_config)\nexcept ImportError: pass\n\nfor mod in search_modules:\n    for name, obj in inspect.getmembers(mod):\n        if inspect.isclass(obj) and \"TrainingConfig\" in name:\n            TrainingConfigClass = obj\n            break\n    if TrainingConfigClass: break\n\n# --- FIX 3: Roles ---\ndef get_valid_role(options):\n    for name in options:\n        if hasattr(tunix.Role, name):\n            return getattr(tunix.Role, name)\n    return options[0]\n\nACTOR_ROLE = get_valid_role([\"ACTOR_TRAIN\", \"ACTOR\", \"POLICY\"])\nCRITIC_ROLE = get_valid_role([\"CRITIC_TRAIN\", \"CRITIC\", \"VALUE\"])\nROLLOUT_ROLE = get_valid_role([\"ROLLOUT\", \"INFERENCE\"]) \nREFERENCE_ROLE = get_valid_role([\"REFERENCE\", \"REF\"])\n\n# =============================================================================\n# STEP 2: Configure Training (The Deep Universal Mock)\n# =============================================================================\nprint(\"2. Building Training Configuration...\")\n\ntrain_args = {\n    \"total_steps\": MAX_STEPS,\n    \"save_every_steps\": 100,\n    \"checkpoint_dir\": CKPT_DIR,\n    \"rollout_micro_batch_size\": MINI_BATCH_SIZE,\n    \"compute_logps_micro_batch_size\": MINI_BATCH_SIZE,\n    \"train_micro_batch_size\": MINI_BATCH_SIZE,\n    # Pass LR here so the mock can use it\n    \"learning_rate\": LEARNING_RATE,\n    \"weight_decay\": WEIGHT_DECAY\n}\n\nif TrainingConfigClass:\n    # --- PATH A: Use Real Class ---\n    # (We assume Real Class handles its own optimizer creation if found)\n    training_config = TrainingConfigClass(**train_args)\nelse:\n    # --- PATH B: Deep Universal Mock (The Fix) ---\n    print(\"   ‚ö†Ô∏è TrainingConfig not found. Using Deep Universal Mock.\")\n    \n    class DeepUniversalTrainingConfig:\n        def __init__(self, **kwargs):\n            # 1. Absorb provided args\n            self.__dict__.update(kwargs)\n            \n            # FIX 1: Add Missing Scalar Attributes (The Error You Saw)\n            if not hasattr(self, 'gradient_accumulation_steps'):\n                self.gradient_accumulation_steps = 1\n            if not hasattr(self, 'max_grad_norm'):\n                self.max_grad_norm = 1.0\n            if not hasattr(self, 'checkpoint_root_directory'):\n                self.checkpoint_root_directory = kwargs.get('checkpoint_dir', CKPT_DIR)\n            if not hasattr(self, 'metrics_logging_options'):\n                self.metrics_logging_options = None\n\n            # FIX 2: Create REAL Optimizers (The Hidden Trap)\n            # The library expects an actual Optax object, not a config dict.\n            lr = kwargs.get('learning_rate', 2e-6)\n            wd = kwargs.get('weight_decay', 0.01)\n            \n            # Create a standard AdamW optimizer\n            # We wrap it in a chain to simulate a real setup\n            real_optimizer = optax.chain(\n                optax.clip_by_global_norm(1.0),\n                optax.adamw(learning_rate=lr, weight_decay=wd)\n            )\n            \n            self.actor_optimizer = real_optimizer\n            self.critic_optimizer = real_optimizer\n            \n            # Keep the config object too, just in case\n            class MockOptConfig: pass\n            self.optimizer_config = MockOptConfig()\n            self.optimizer_config.learning_rate = lr\n            self.optimizer_config.weight_decay = wd\n\n    training_config = DeepUniversalTrainingConfig(**train_args)\n\n# =============================================================================\n# STEP 3: Configure Cluster (Static Memory)\n# =============================================================================\nprint(\"3. Building Cluster Configuration...\")\n\nrole_to_mesh_dict = {\n    ACTOR_ROLE: mesh,\n    CRITIC_ROLE: mesh,\n    ROLLOUT_ROLE: mesh,\n    REFERENCE_ROLE: mesh\n}\n\ncluster_config = ClusterConfigClass(\n    role_to_mesh=role_to_mesh_dict,\n    training_config=training_config,\n    \n    # STATIC MEMORY MODE (Crucial for Stability)\n    offload_to_cpu=False,\n    \n    rollout_config=base_rollout.RolloutConfig(\n        max_tokens_to_generate=MAX_GENERATION_LENGTH,\n        max_prompt_length=MAX_PROMPT_LENGTH,\n        temperature=1.0,\n        top_k=50,\n        top_p=0.95,\n        eos_tokens=[tokenizer.eos_token_id, 107],\n        seed=RANDOM_SEED,\n    ),\n)\n\n# =============================================================================\n# STEP 4: Instantiate Components\n# =============================================================================\nprint(\"4. Instantiating Cluster...\")\n\ncluster = rl_cluster.RLCluster(\n    cluster_config=cluster_config,\n    tokenizer=tokenizer,\n    actor=actor,\n    reference=ref_model,\n    critic=None,\n    reward=None\n)\n\nprint(\"5. Building GRPO Trainer...\")\nalgorithm_config = grpo_learner.GRPOConfig(\n    num_generations=NUM_GENERATIONS,\n    epsilon=EPSILON,\n    beta=BETA,\n    num_iterations=NUM_ITERATIONS,\n)\n\ngrpo_trainer = grpo_learner.GRPOLearner(\n    rl_cluster=cluster,\n    algorithm_config=algorithm_config,\n    training_config=training_config, \n    reward_functions=[\n        format_reward_func,\n        logic_reward_func,\n        accuracy_reward_func\n    ],\n    reward_weights=[\n        REWARD_WEIGHT_FORMAT,\n        REWARD_WEIGHT_LOGIC,\n        REWARD_WEIGHT_ACCURACY\n    ]\n)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"‚úÖ GRPO TRAINER READY\")\nprint(\"=\"*60)\nmonitor.print_summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T05:00:30.327156Z","iopub.execute_input":"2026-01-21T05:00:30.327424Z","iopub.status.idle":"2026-01-21T05:00:31.103237Z","shell.execute_reply.started":"2026-01-21T05:00:30.327406Z","shell.execute_reply":"2026-01-21T05:00:31.102001Z"}},"outputs":[{"name":"stderr","text":"WARNING:absl:Reference model and actor model are colocated but do not share the same backbone. This will result in an unnecessary model copy and increased HBM usage.\n","output_type":"stream"},{"name":"stdout","text":"1. Initializing Tunix components...\n2. Building Training Configuration...\n   ‚ö†Ô∏è TrainingConfig not found. Using Universal Mock.\n3. Building Cluster Configuration...\n4. Instantiating Cluster...\n","output_type":"stream"},{"traceback":["\u001b[31m---------------------------------------------------------------------------\u001b[39m","\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)","\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 157\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# STEP 4: Instantiate Components\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m4. Instantiating Cluster...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m cluster = \u001b[43mrl_cluster\u001b[49m\u001b[43m.\u001b[49m\u001b[43mRLCluster\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcluster_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcluster_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m    \u001b[49m\u001b[43mactor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreference\u001b[49m\u001b[43m=\u001b[49m\u001b[43mref_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcritic\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    164\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m5. Building GRPO Trainer...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    167\u001b[39m algorithm_config = grpo_learner.GRPOConfig(\n\u001b[32m    168\u001b[39m     num_generations=NUM_GENERATIONS,\n\u001b[32m    169\u001b[39m     epsilon=EPSILON,\n\u001b[32m    170\u001b[39m     beta=BETA,\n\u001b[32m    171\u001b[39m     num_iterations=NUM_ITERATIONS,\n\u001b[32m    172\u001b[39m )\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/tunix/rl/rl_cluster.py:258\u001b[39m, in \u001b[36mRLCluster.__init__\u001b[39m\u001b[34m(self, actor, critic, reference, reward, tokenizer, cluster_config, perf_config)\u001b[39m\n\u001b[32m    255\u001b[39m     devices.extend(mesh.devices.flatten().tolist())\n\u001b[32m    256\u001b[39m   \u001b[38;5;28mself\u001b[39m._perf = perf_trace.PerfTracer(devices, perf_config.custom_export_fn)\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_cluster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    259\u001b[39m gc.collect()\n\u001b[32m    261\u001b[39m \u001b[38;5;66;03m# NB: global steps should be adjusted properly based on the actual RL\u001b[39;00m\n\u001b[32m    262\u001b[39m \u001b[38;5;66;03m# algorithm. E.g. when loading from a checkpoint with additional inner loops\u001b[39;00m\n\u001b[32m    263\u001b[39m \u001b[38;5;66;03m# that update the model, we should properly update the global steps.\u001b[39;00m\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/tunix/rl/rl_cluster.py:512\u001b[39m, in \u001b[36mRLCluster._init_cluster\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    508\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m actor_config.checkpoint_root_directory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    509\u001b[39m   actor_config.checkpoint_root_directory = os.path.join(\n\u001b[32m    510\u001b[39m       actor_config.checkpoint_root_directory, \u001b[33m\"\u001b[39m\u001b[33mactor\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    511\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m512\u001b[39m \u001b[38;5;28mself\u001b[39m._actor_trainer = \u001b[43mrl_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_actor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcluster_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mactor_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mactor_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_checkpoint_metadata_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mglobal_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mglobal_steps\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m    518\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# offset by 1 since global_step is incremented after the training loop in rl_learner. # pylint: disable=line-too-long\u001b[39;49;00m\n\u001b[32m    519\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetrics_logger\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_rl_metrics_logger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m    \u001b[49m\u001b[43mperf_tracer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_perf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.rollout_actor\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.train_actor\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/tunix/rl/trainer.py:40\u001b[39m, in \u001b[36mTrainer.__init__\u001b[39m\u001b[34m(self, model, optimizer, training_config, custom_checkpoint_metadata_fn, metrics_logger, perf_tracer)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     32\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     33\u001b[39m     model: nnx.Module,\n\u001b[32m   (...)\u001b[39m\u001b[32m     38\u001b[39m     perf_tracer: Optional[perf_trace.Tracer] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     39\u001b[39m ):\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m   \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmetrics_logger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m      \u001b[49m\u001b[43mperf_tracer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m   \u001b[38;5;28mself\u001b[39m.rl_metrics_to_log = {}  \u001b[38;5;66;03m# Metric name -> key in aux.\u001b[39;00m\n\u001b[32m     48\u001b[39m   \u001b[38;5;28mself\u001b[39m.tqdm_metrics_to_display = []\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/tunix/sft/peft_trainer.py:195\u001b[39m, in \u001b[36mPeftTrainer.__init__\u001b[39m\u001b[34m(self, model, optimizer, training_config, metrics_logger, perf_tracer)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28mself\u001b[39m.config = training_config\n\u001b[32m    194\u001b[39m \u001b[38;5;28mself\u001b[39m._lora_enabled = utils.is_lora_enabled(\u001b[38;5;28mself\u001b[39m.model)\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtraining_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    196\u001b[39m   optimizer = optax.MultiSteps(\n\u001b[32m    197\u001b[39m       optimizer, training_config.gradient_accumulation_steps\n\u001b[32m    198\u001b[39m   )\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lora_enabled:\n","\u001b[31mAttributeError\u001b[39m: 'UniversalTrainingConfig' object has no attribute 'gradient_accumulation_steps'"],"ename":"AttributeError","evalue":"'UniversalTrainingConfig' object has no attribute 'gradient_accumulation_steps'","output_type":"error"}],"execution_count":12},{"cell_type":"markdown","source":"## üöÄ Cell 11: Training Loop\n*First steps take 10-15 minutes for JIT compilation*","metadata":{}},{"cell_type":"code","source":"print(\"=\"*60)\nprint(\"üöÄ STARTING GRPO TRAINING\")\nprint(\"=\"*60)\nprint(f\"\")\nprint(f\"Configuration:\")\nprint(f\"  ‚Ä¢ Steps: {MAX_STEPS}\")\nprint(f\"  ‚Ä¢ Generations per prompt: {NUM_GENERATIONS}\")\nprint(f\"  ‚Ä¢ Iterations per batch: {NUM_ITERATIONS}\")\nprint(f\"  ‚Ä¢ Train samples: {NUM_TRAIN_SAMPLES}\")\nprint(f\"\")\nprint(f\"Reward Weights:\")\nprint(f\"  ‚Ä¢ Format: {REWARD_WEIGHT_FORMAT*100}%\")\nprint(f\"  ‚Ä¢ Logic: {REWARD_WEIGHT_LOGIC*100}%\")\nprint(f\"  ‚Ä¢ Accuracy: {REWARD_WEIGHT_ACCURACY*100}%\")\nprint(f\"\")\nprint(\"=\"*60)\nprint(\"\\n‚è≥ First steps will take 10-15 minutes for JIT compilation...\")\nprint(\"   Subsequent steps will be much faster.\\n\")\n\nmonitor.print_summary()\nstart_time = time.time()\n\n# Run training\nwith mesh:\n    grpo_trainer.train(train_dataset, val_dataset)\n\nelapsed = time.time() - start_time\nprint(\"\\n\" + \"=\"*60)\nprint(\"üéâ TRAINING COMPLETE\")\nprint(\"=\"*60)\nprint(f\"   Total time: {elapsed/60:.1f} minutes\")\nprint(f\"   Steps completed: {MAX_STEPS}\")\nmonitor.print_summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üíæ Cell 12: Save Model","metadata":{}},{"cell_type":"code","source":"print(\"Saving trained model...\")\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nwith mesh:\n    _, actor_state = nnx.split(actor)\n    checkpointer = ocp.StandardCheckpointer()\n    save_path = os.path.join(OUTPUT_DIR, \"actor_state\")\n    checkpointer.save(save_path, actor_state)\n\nprint(f\"‚úÖ Model saved to {OUTPUT_DIR}\")\nprint(f\"   Checkpoint: {save_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üß™ Cell 13: Test Inference","metadata":{}},{"cell_type":"code","source":"test_questions = [\n    (\"If a store sells 150 apples at $4 each, what is the total revenue?\", \"600\"),\n    (\"A train travels 120 miles in 2 hours. What is its speed in miles per hour?\", \"60\"),\n    (\"Sarah has 24 cookies. She gives 1/3 to her brother and 1/4 to her sister. How many cookies does she have left?\", \"10\"),\n]\n\nprint(\"=\"*60)\nprint(\"üß™ TESTING TRAINED MODEL\")\nprint(\"=\"*60 + \"\\n\")\n\nwith mesh:\n    sampler = sampler_lib.Sampler(\n        model=actor,\n        tokenizer=tokenizer,\n        max_tokens=MAX_GENERATION_LENGTH\n    )\n    \n    for i, (question, expected) in enumerate(test_questions, 1):\n        prompt = format_prompt(question)\n        output = sampler.generate(prompt)\n        \n        print(f\"Question {i}: {question}\")\n        print(f\"Expected: {expected}\")\n        print(f\"Output:\")\n        print(output[:800])\n        print(\"-\" * 60 + \"\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üìã Cell 14: Final Summary","metadata":{}},{"cell_type":"code","source":"print(\"=\"*60)\nprint(\"üèÜ PRODUCTION REASONING MODEL COMPLETE\")\nprint(\"=\"*60)\nprint(f\"\")\nprint(f\"Architecture:\")\nprint(f\"  ‚Ä¢ Hardware: TPU v5e-8 (2√ó4 mesh)\")\nprint(f\"  ‚Ä¢ Model: {MODEL_HF_NAME}\")\nprint(f\"  ‚Ä¢ Fine-Tuning: LoRA (rank={LORA_RANK}, alpha={LORA_ALPHA})\")\nprint(f\"  ‚Ä¢ Targets: Attention + MLP layers\")\nprint(f\"\")\nprint(f\"GRPO Configuration:\")\nprint(f\"  ‚Ä¢ Generations (G): {NUM_GENERATIONS}\")\nprint(f\"  ‚Ä¢ Iterations (Œº): {NUM_ITERATIONS}\")\nprint(f\"  ‚Ä¢ KL Penalty (Œ≤): {BETA}\")\nprint(f\"  ‚Ä¢ Clipping (Œµ): {EPSILON}\")\nprint(f\"\")\nprint(f\"Reward Weights:\")\nprint(f\"  ‚Ä¢ Format: {REWARD_WEIGHT_FORMAT*100}%\")\nprint(f\"  ‚Ä¢ Logic: {REWARD_WEIGHT_LOGIC*100}%\")\nprint(f\"  ‚Ä¢ Accuracy: {REWARD_WEIGHT_ACCURACY*100}%\")\nprint(f\"  ‚Ä¢ Self-Correction: bonus\")\nprint(f\"  ‚Ä¢ Length: bonus\")\nprint(f\"\")\nprint(f\"Training:\")\nprint(f\"  ‚Ä¢ Steps: {MAX_STEPS}\")\nprint(f\"  ‚Ä¢ Samples: {NUM_TRAIN_SAMPLES} train, {NUM_TEST_SAMPLES} test\")\nprint(f\"  ‚Ä¢ Output: {OUTPUT_DIR}\")\nprint(f\"\")\nmonitor.print_summary()\nprint(\"=\"*60)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}