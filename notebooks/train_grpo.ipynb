{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"TPU","kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":85992,"sourceType":"modelInstanceVersion","modelInstanceId":72251,"modelId":76277}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üß† Gemma2 Reasoning GRPO - Full Production\n\n**Google Tunix Hackathon 2026**\n\n### Core Architecture:\n- **Hardware:** TPU v5e-8 (2 Data x 4 Tensor Mesh)\n- **Model:** Gemma 2 2B-IT (CPU-Offloaded bf16 Init)\n- **Fine-Tuning:** LoRA on Attention + MLP layers\n- **Strategy:** GRPO with 16 Parallel Generations per prompt\n- **Rewards:** Format (25%) + Logic (30%) + Accuracy (45%) + Self-Correction & Length Regularization","metadata":{}},{"cell_type":"markdown","source":"## Cell 1: Environment Setup","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\nimport glob\nimport time\n\n# =============================================================================\n# FORCE RESET: Smart Cleanup of ALL old markers\n# =============================================================================\n# This defines the NEW marker we want to create after success\nCURRENT_MARKER = \"/kaggle/working/.setup_complete_gtg\"\n\nprint(\"üßπ Cleaning up old installation markers...\")\n\n# Use glob to find ANY file starting with \".setup_complete\"\nold_markers = glob.glob(\"/kaggle/working/.setup_complete*\")\n\nfor marker in old_markers:\n    # Don't delete the current one if it essentially already exists (though we usually want to overwrite)\n    try:\n        os.remove(marker)\n        print(f\"   üóëÔ∏è Deleted old marker: {marker}\")\n    except OSError as e:\n        print(f\"   ‚ö†Ô∏è Could not delete {marker}: {e}\")\n\nprint(\"=\"*60)\nprint(\"üîÑ INSTALLING DEPENDENCIES\")\nprint(\"=\"*60)\n\n%pip install --upgrade pip -q\n\n# 1. Clean previous installations\n    print(\"\\nüßπ Cleaning previous installations...\")\n    %pip uninstall -y tunix google-tunix flax qwix 2>/dev/null\n    \n    # 2. Install JAX for TPU\n    print(\"\\n‚¨áÔ∏è Installing JAX/TPU stack...\")\n    %pip install -q \"jax[tpu]>=0.8.0\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n    \n    # 3. Install Tunix from PyPI (STABLE - recommended)\n    print(\"\\n‚¨áÔ∏è Installing Tunix (stable from PyPI)...\")\n    %pip install -q \"google-tunix[prod]\"\n    \n    # 4. Install Qwix and Flax\n    print(\"\\n‚¨áÔ∏è Installing Qwix and Flax...\")\n    %pip install -q git+https://github.com/google/qwix\n    %pip install -q git+https://github.com/google/flax\n    \n    # 5. Install other dependencies\n    print(\"\\n‚¨áÔ∏è Installing other dependencies...\")\n    %pip install -q kagglehub transformers grain huggingface_hub orbax-checkpoint\n    %pip install -q \"numpy>=2.0.0\" \"pyarrow>=17.0.0\" \"datasets>=2.21.0\"\n    \n    # Create marker\n    with open(MARKER, \"w\") as f:\n        f.write(\"done\")\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"‚úÖ INSTALLATION COMPLETE\")\n    print(\"‚ö†Ô∏è  PLEASE RESTART KERNEL NOW (‚ü≥ Button)!\")\n    print(\"=\"*60)\nelse:\n    print(\"‚úÖ Dependencies already installed. Proceeding...\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 2: Verify Installation","metadata":{}},{"cell_type":"code","source":"import sys\n\nprint(\"üîç Verifying Tunix Installation...\\n\")\n\ntry:\n    # Check tunix imports\n    from tunix.rl import rl_cluster as rl_cluster_lib\n    from tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\n    from tunix.rl.rollout import base_rollout\n    from tunix.sft import metrics_logger\n    \n    print(\"‚úÖ tunix.rl.rl_cluster imported\")\n    print(\"‚úÖ GRPOConfig, GRPOLearner imported\")\n    print(\"‚úÖ base_rollout imported\")\n    print(\"‚úÖ metrics_logger imported\")\n    \n    # Verify correct classes exist\n    assert hasattr(rl_cluster_lib, 'ClusterConfig'), \"ClusterConfig not found!\"\n    assert hasattr(rl_cluster_lib, 'RLTrainingConfig'), \"RLTrainingConfig not found!\"\n    assert hasattr(rl_cluster_lib, 'Role'), \"Role enum not found!\"\n    assert hasattr(rl_cluster_lib, 'RLCluster'), \"RLCluster not found!\"\n    \n    print(\"\\n‚úÖ All required classes found:\")\n    print(f\"   - ClusterConfig: {rl_cluster_lib.ClusterConfig}\")\n    print(f\"   - RLTrainingConfig: {rl_cluster_lib.RLTrainingConfig}\")\n    print(f\"   - Role: {list(rl_cluster_lib.Role)}\")\n    \n    # Check RLTrainingConfig has the fields we need\n    import inspect\n    sig = inspect.signature(rl_cluster_lib.RLTrainingConfig)\n    params = list(sig.parameters.keys())\n    print(f\"\\n‚úÖ RLTrainingConfig parameters: {params[:10]}...\")\n    \n    if 'actor_optimizer' in params:\n        print(\"‚úÖ 'actor_optimizer' field exists (required)\")\n    if 'gradient_accumulation_steps' in params:\n        print(\"‚úÖ 'gradient_accumulation_steps' field exists (optional)\")\n        \n    print(\"\\n\" + \"=\"*60)\n    print(\"üéâ TUNIX INSTALLATION VERIFIED SUCCESSFULLY!\")\n    print(\"=\"*60)\n    \nexcept ImportError as e:\n    print(f\"‚ùå Import Error: {e}\")\n    print(\"\\nPlease run Cell 1 and restart the kernel.\")\n    sys.exit(1)\nexcept AssertionError as e:\n    print(f\"‚ùå Assertion Error: {e}\")\n    print(\"\\nYour tunix version may be incompatible. Try reinstalling.\")\n    sys.exit(1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 3: TPU Setup & Memory Monitor","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# TPU SETUP & MEMORY MONITORING\n# =============================================================================\nimport jax\nimport jax.numpy as jnp\nfrom jax.sharding import Mesh\nimport time\n\nprint(\"=\"*60)\nprint(\"‚ö° TPU INITIALIZATION\")\nprint(\"=\"*60)\n\n# Check devices\ndevices = jax.devices()\nNUM_TPUS = len(devices)\nprint(f\"\\n‚úÖ Found {NUM_TPUS} TPU devices:\")\nfor i, d in enumerate(devices):\n    print(f\"   Device {i}: {d}\")\n\n# Determine mesh shape based on TPU count\nif NUM_TPUS == 8:\n    MESH_SHAPE = (2, 4)  # 2 FSDP x 4 TP\nelif NUM_TPUS == 4:\n    MESH_SHAPE = (2, 2)\nelif NUM_TPUS == 1:\n    MESH_SHAPE = (1, 1)\nelse:\n    raise ValueError(f\"Unsupported TPU count: {NUM_TPUS}\")\n\nMESH_AXIS_NAMES = (\"fsdp\", \"tp\")\n\nprint(f\"\\n‚úÖ Mesh configuration: {MESH_SHAPE} ({MESH_AXIS_NAMES})\")\n\n# Memory monitoring class\nclass TPUMemoryMonitor:\n    def __init__(self):\n        self.devices = jax.devices()\n    \n    def get_memory_stats(self):\n        stats = []\n        for i, device in enumerate(self.devices):\n            try:\n                mem = device.memory_stats()\n                if mem:\n                    used_gb = mem.get('bytes_in_use', 0) / 1e9\n                    limit_gb = mem.get('bytes_limit', 0) / 1e9\n                    pct = 100 * used_gb / limit_gb if limit_gb > 0 else 0\n                    stats.append((i, used_gb, limit_gb, pct))\n            except:\n                pass\n        return stats\n    \n    def print_summary(self):\n        print(\"\\nüìä TPU Memory Usage:\")\n        stats = self.get_memory_stats()\n        if stats:\n            for i, used, limit, pct in stats:\n                bar = \"‚ñà\" * int(pct/5) + \"‚ñë\" * (20 - int(pct/5))\n                status = \"‚ö†Ô∏è\" if pct > 80 else \"‚úÖ\"\n                print(f\"   TPU {i}: [{bar}] {used:.1f}/{limit:.1f} GB ({pct:.1f}%) {status}\")\n        else:\n            print(\"   Could not retrieve memory stats\")\n\nmonitor = TPUMemoryMonitor()\nmonitor.print_summary()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"‚úÖ TPU READY\")\nprint(\"=\"*60)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 4: Hyperparameters.","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# HYPERPARAMETERS - Memory Safe for TPU v5e (128GB)\n# =============================================================================\nfrom dataclasses import dataclass\n\n@dataclass\nclass Config:\n    # === Model ===\n    MODEL_HF_NAME: str = \"google/gemma-2-2b-it\"\n    \n    # === LoRA ===\n    LORA_RANK: int = 32\n    LORA_ALPHA: float = 32.0\n    \n    # === GRPO Algorithm ===\n    NUM_GENERATIONS: int = 2      # G in paper (keep low for memory)\n    NUM_ITERATIONS: int = 1       # Œº in paper\n    BETA: float = 0.04            # KL penalty coefficient\n    EPSILON: float = 0.2          # Clipping parameter\n    \n    # === Generation ===\n    MAX_PROMPT_LENGTH: int = 256\n    MAX_GENERATION_LENGTH: int = 300  # Your requirement\n    TEMPERATURE: float = 0.9      # High for diverse responses\n    TOP_P: float = 1.0\n    TOP_K: int = 50\n    \n    # === Training (MEMORY SAFE) ===\n    TRAIN_MICRO_BATCH_SIZE: int = 1   # Minimum for safety\n    MINI_BATCH_SIZE: int = 1\n    \n    # === Optimizer ===\n    LEARNING_RATE: float = 3e-6\n    WARMUP_RATIO: float = 0.1\n    WEIGHT_DECAY: float = 0.1\n    ADAM_B1: float = 0.9\n    ADAM_B2: float = 0.99\n    MAX_GRAD_NORM: float = 0.1\n    \n    # === Schedule ===\n    MAX_STEPS: int = 100          # Adjust based on dataset\n    EVAL_EVERY_N_STEPS: int = 20\n    \n    # === Dataset ===\n    NUM_TRAIN_SAMPLES: int = 500\n    NUM_TEST_SAMPLES: int = 50\n    TRAIN_FRACTION: float = 1.0\n    \n    # === Reward Weights ===\n    REWARD_WEIGHT_FORMAT: float = 0.25\n    REWARD_WEIGHT_LOGIC: float = 0.30\n    REWARD_WEIGHT_ACCURACY: float = 0.45\n    \n    # === Paths ===\n    OUTPUT_DIR: str = \"/kaggle/working/grpo_output\"\n    CKPT_DIR: str = \"/kaggle/working/checkpoints\"\n\ncfg = Config()\n\nprint(\"=\"*60)\nprint(\"üìù CONFIGURATION\")\nprint(\"=\"*60)\nprint(f\"\\nModel: {cfg.MODEL_HF_NAME}\")\nprint(f\"LoRA: rank={cfg.LORA_RANK}, alpha={cfg.LORA_ALPHA}\")\nprint(f\"\\nGRPO: G={cfg.NUM_GENERATIONS}, Œº={cfg.NUM_ITERATIONS}, Œ≤={cfg.BETA}, Œµ={cfg.EPSILON}\")\nprint(f\"Generation: max_len={cfg.MAX_GENERATION_LENGTH}, temp={cfg.TEMPERATURE}\")\nprint(f\"\\nTraining: batch={cfg.TRAIN_MICRO_BATCH_SIZE}, steps={cfg.MAX_STEPS}\")\nprint(f\"Optimizer: lr={cfg.LEARNING_RATE}, warmup={cfg.WARMUP_RATIO*100}%\")\nprint(f\"\\nRewards: format={cfg.REWARD_WEIGHT_FORMAT}, logic={cfg.REWARD_WEIGHT_LOGIC}, acc={cfg.REWARD_WEIGHT_ACCURACY}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 5: Data Loading (GSM8K)","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# LOAD DATASET\n# =============================================================================\nimport re\nfrom datasets import load_dataset\n\nprint(\"üìö Loading GSM8K dataset...\")\n\n# Load dataset\ngsm8k = load_dataset(\"openai/gsm8k\", \"main\")\n\ndef extract_answer(solution: str) -> str:\n    \"\"\"Extract the final numerical answer from GSM8K solution.\"\"\"\n    # GSM8K answers are formatted as: #### <number>\n    match = re.search(r'####\\s*([\\d,\\.\\-]+)', solution)\n    if match:\n        return match.group(1).replace(',', '')\n    return \"\"\n\ndef format_prompt(question: str) -> str:\n    \"\"\"Format question with reasoning instructions.\"\"\"\n    return f\"\"\"You are a helpful math tutor. Solve this problem step by step.\n\nProblem: {question}\n\nThink through this carefully. Show your reasoning inside <reasoning></reasoning> tags, then give your final numerical answer inside <answer></answer> tags.\"\"\"\n\n# Prepare datasets\ndef prepare_dataset(split, max_samples):\n    data = gsm8k[split].select(range(min(max_samples, len(gsm8k[split]))))\n    prompts = []\n    answers = []\n    for item in data:\n        prompts.append(format_prompt(item['question']))\n        answers.append(extract_answer(item['answer']))\n    return prompts, answers\n\ntrain_prompts, train_answers = prepare_dataset('train', cfg.NUM_TRAIN_SAMPLES)\ntest_prompts, test_answers = prepare_dataset('test', cfg.NUM_TEST_SAMPLES)\n\nprint(f\"\\n‚úÖ Loaded {len(train_prompts)} training samples\")\nprint(f\"‚úÖ Loaded {len(test_prompts)} test samples\")\n\n# Show example\nprint(\"\\n\" + \"=\"*60)\nprint(\"Example prompt:\")\nprint(\"=\"*60)\nprint(train_prompts[0][:500] + \"...\")\nprint(f\"\\nExpected answer: {train_answers[0]}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 6: Load Model & Tokenizer","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# LOAD MODEL & TOKENIZER\n# =============================================================================\nimport os\nimport kagglehub\nfrom flax import nnx\n\nprint(\"=\"*60)\nprint(\"ü§ñ LOADING MODEL & TOKENIZER\")\nprint(\"=\"*60)\n\n# Download model from Kaggle\nprint(\"\\n‚¨áÔ∏è Downloading model from Kaggle...\")\nmodel_path = kagglehub.model_download(\"google/gemma-2/flax/gemma2-2b-it\")\nprint(f\"‚úÖ Model downloaded to: {model_path}\")\n\n# Import tunix model components\nfrom tunix.models.gemma2 import model as gemma_lib\nfrom tunix.models.gemma2 import params_safetensors as params_safetensors_lib\nfrom tunix.models.gemma2 import transformer_config\nfrom tunix.generate import tokenizer_adapter as tokenizer_lib\n\n# Load tokenizer\nprint(\"\\n‚¨áÔ∏è Loading tokenizer...\")\ntokenizer = tokenizer_lib.HuggingFaceTokenizerAdapter.from_pretrained(cfg.MODEL_HF_NAME)\nprint(f\"‚úÖ Tokenizer loaded. Vocab size: {tokenizer.vocab_size}\")\n\n# Get EOS tokens\nEOS_TOKENS = [tokenizer.eos_token_id] if tokenizer.eos_token_id else [1]\nprint(f\"‚úÖ EOS tokens: {EOS_TOKENS}\")\n\n# Get model config\nmodel_config = transformer_config.get_model_config_by_name(\"gemma2-2b\")\nprint(f\"‚úÖ Model config: {model_config.num_layers} layers, {model_config.embed_dim} embed dim\")\n\nmonitor.print_summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 7: Create Mesh & Load Weights","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# CREATE MESH & LOAD MODEL WEIGHTS\n# =============================================================================\nimport os\n\nprint(\"=\"*60)\nprint(\"üîß CREATING MESH & LOADING WEIGHTS\")\nprint(\"=\"*60)\n\n# Create JAX mesh\nmesh = jax.make_mesh(\n    MESH_SHAPE,\n    MESH_AXIS_NAMES,\n    axis_types=(jax.sharding.AxisType.Auto,) * len(MESH_SHAPE)\n)\nprint(f\"\\n‚úÖ Created mesh: {mesh}\")\n\n# Load model weights within mesh context\nprint(\"\\n‚¨áÔ∏è Loading model weights (this may take a few minutes)...\")\n\nwith mesh:\n    gemma = params_safetensors_lib.create_model_from_safe_tensors(\n        os.path.abspath(model_path),\n        model_config,\n        mesh,\n        dtype=jnp.bfloat16\n    )\n\nprint(\"‚úÖ Model weights loaded\")\nmonitor.print_summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 8: Apply LORA","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# APPLY LoRA\n# =============================================================================\nimport qwix\n\nprint(\"=\"*60)\nprint(\"üéØ APPLYING LoRA\")\nprint(\"=\"*60)\n\n# LoRA target layers - attention and MLP\nLORA_TARGETS = [\n    \".*attn.q_proj.*\",\n    \".*attn.k_proj.*\",\n    \".*attn.v_proj.*\",\n    \".*attn.o_proj.*\",\n    \".*mlp.gate_proj.*\",\n    \".*mlp.up_proj.*\",\n    \".*mlp.down_proj.*\",\n]\n\nprint(f\"\\nLoRA config:\")\nprint(f\"  Rank: {cfg.LORA_RANK}\")\nprint(f\"  Alpha: {cfg.LORA_ALPHA}\")\nprint(f\"  Targets: {len(LORA_TARGETS)} layer patterns\")\n\nwith mesh:\n    # Create LoRA actor (trainable)\n    actor = qwix.apply_lora(\n        gemma,\n        rank=cfg.LORA_RANK,\n        alpha=cfg.LORA_ALPHA,\n        target_modules=LORA_TARGETS,\n    )\n    \n    # Keep reference model frozen (no LoRA)\n    reference = gemma\n\nprint(\"\\n‚úÖ LoRA applied to actor model\")\nprint(\"‚úÖ Reference model kept frozen\")\n\n# Count parameters\ndef count_params(model):\n    params = nnx.state(model, nnx.Param)\n    return sum(p.size for p in jax.tree_util.tree_leaves(params))\n\nprint(f\"\\nParameter counts:\")\ntry:\n    actor_params = count_params(actor)\n    ref_params = count_params(reference)\n    print(f\"  Actor (with LoRA): ~{actor_params/1e9:.2f}B\")\n    print(f\"  Reference: ~{ref_params/1e9:.2f}B\")\nexcept:\n    print(\"  (Could not count parameters)\")\n\nmonitor.print_summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 9: Reward Functions","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# REWARD FUNCTIONS\n# =============================================================================\nimport re\nfrom typing import List, Optional\n\nprint(\"=\"*60)\nprint(\"üèÜ DEFINING REWARD FUNCTIONS\")\nprint(\"=\"*60)\n\ndef match_format_exactly(prompts: List[str], completions: List[str], **kwargs) -> List[float]:\n    \"\"\"\n    Reward for exact format: <reasoning>...</reasoning><answer>...</answer>\n    Weight: 25%\n    \"\"\"\n    rewards = []\n    pattern = r'<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>'\n    for completion in completions:\n        if re.search(pattern, completion, re.DOTALL):\n            rewards.append(1.0 * cfg.REWARD_WEIGHT_FORMAT)\n        else:\n            rewards.append(0.0)\n    return rewards\n\ndef match_format_approximately(prompts: List[str], completions: List[str], **kwargs) -> List[float]:\n    \"\"\"\n    Partial reward for having reasoning and answer tags.\n    \"\"\"\n    rewards = []\n    for completion in completions:\n        score = 0.0\n        if '<reasoning>' in completion and '</reasoning>' in completion:\n            score += 0.5\n        if '<answer>' in completion and '</answer>' in completion:\n            score += 0.5\n        rewards.append(score * cfg.REWARD_WEIGHT_FORMAT * 0.5)  # Half weight for partial\n    return rewards\n\ndef check_answer(prompts: List[str], completions: List[str], \n                 expected_answers: Optional[List[str]] = None, **kwargs) -> List[float]:\n    \"\"\"\n    Reward for correct numerical answer.\n    Weight: 45%\n    \"\"\"\n    if expected_answers is None:\n        return [0.0] * len(completions)\n    \n    rewards = []\n    for completion, expected in zip(completions, expected_answers):\n        match = re.search(r'<answer>(.*?)</answer>', completion, re.DOTALL)\n        if match:\n            predicted = match.group(1).strip().replace(',', '')\n            expected_clean = expected.strip().replace(',', '')\n            \n            # Exact match\n            if predicted == expected_clean:\n                rewards.append(1.0 * cfg.REWARD_WEIGHT_ACCURACY)\n            # Try numeric comparison\n            else:\n                try:\n                    pred_num = float(predicted)\n                    exp_num = float(expected_clean)\n                    if abs(pred_num - exp_num) < 0.01:  # Close enough\n                        rewards.append(0.9 * cfg.REWARD_WEIGHT_ACCURACY)\n                    elif abs(pred_num - exp_num) / max(abs(exp_num), 1) < 0.1:  # Within 10%\n                        rewards.append(0.5 * cfg.REWARD_WEIGHT_ACCURACY)\n                    else:\n                        rewards.append(0.0)\n                except:\n                    rewards.append(0.0)\n        else:\n            rewards.append(0.0)\n    return rewards\n\ndef check_numbers(prompts: List[str], completions: List[str], **kwargs) -> List[float]:\n    \"\"\"\n    Reward for showing mathematical work (numbers, operations).\n    Weight: 30% (logic)\n    \"\"\"\n    rewards = []\n    for completion in completions:\n        score = 0.0\n        reasoning_match = re.search(r'<reasoning>(.*?)</reasoning>', completion, re.DOTALL)\n        if reasoning_match:\n            reasoning = reasoning_match.group(1)\n            \n            # Has numbers\n            if re.search(r'\\d+', reasoning):\n                score += 0.3\n            \n            # Has operations\n            if any(op in reasoning for op in ['+', '-', '*', '/', '=']):\n                score += 0.3\n            \n            # Has step indicators\n            if any(word in reasoning.lower() for word in ['step', 'first', 'then', 'next', 'so', 'therefore']):\n                score += 0.2\n            \n            # Reasonable length (not too short, not too long)\n            if 50 < len(reasoning) < 1500:\n                score += 0.2\n        \n        rewards.append(score * cfg.REWARD_WEIGHT_LOGIC)\n    return rewards\n\nprint(\"\\n‚úÖ Defined 4 reward functions:\")\nprint(f\"   1. match_format_exactly (weight: {cfg.REWARD_WEIGHT_FORMAT*100}%)\")\nprint(f\"   2. match_format_approximately\")\nprint(f\"   3. check_answer (weight: {cfg.REWARD_WEIGHT_ACCURACY*100}%)\")\nprint(f\"   4. check_numbers (weight: {cfg.REWARD_WEIGHT_LOGIC*100}%)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 10: GRPO Trainer Setup","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# GRPO TRAINER SETUP - FIXED TUNIX API\n# =============================================================================\nimport optax\nimport os\n\n# Correct imports for bleeding-edge tunix\nfrom tunix.rl import rl_cluster as rl_cluster_lib\nfrom tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\nfrom tunix.rl.rollout import base_rollout\nfrom tunix.sft import metrics_logger\nfrom orbax import checkpoint as ocp\n\nprint(\"=\"*60)\nprint(\"üöÄ GRPO TRAINER SETUP (FIXED API)\")\nprint(\"=\"*60)\n\n# Create output directories\nos.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\nos.makedirs(cfg.CKPT_DIR, exist_ok=True)\n\n# ---------------------------------------------------------------------------\n# STEP 1: Create REAL Optax Optimizer (NOT a mock!)\n# ---------------------------------------------------------------------------\nprint(\"\\nüìå Step 1: Creating optax optimizer...\")\n\nWARMUP_STEPS = int(cfg.WARMUP_RATIO * cfg.MAX_STEPS)\n\noptimizer = optax.chain(\n    # Gradient clipping (CRITICAL for stability)\n    optax.clip_by_global_norm(max_norm=cfg.MAX_GRAD_NORM),\n    # AdamW with warmup + cosine decay\n    optax.adamw(\n        learning_rate=optax.warmup_cosine_decay_schedule(\n            init_value=0.0,\n            peak_value=cfg.LEARNING_RATE,\n            warmup_steps=WARMUP_STEPS,\n            decay_steps=cfg.MAX_STEPS,\n            end_value=0.0,\n        ),\n        b1=cfg.ADAM_B1,\n        b2=cfg.ADAM_B2,\n        weight_decay=cfg.WEIGHT_DECAY,\n    ),\n)\nprint(f\"   ‚úÖ AdamW optimizer created (lr={cfg.LEARNING_RATE}, warmup={WARMUP_STEPS})\")\n\n# ---------------------------------------------------------------------------\n# STEP 2: Create Metrics Logging Options\n# ---------------------------------------------------------------------------\nprint(\"\\nüìå Step 2: Creating metrics logger...\")\n\nmetrics_logging_options = metrics_logger.MetricsLoggerOptions(\n    log_to_stdout=True,\n    log_to_tensorboard=True,\n    tensorboard_dir=os.path.join(cfg.OUTPUT_DIR, \"tensorboard\"),\n    log_to_wandb=False,  # Disable if causing issues on Kaggle\n)\nprint(\"   ‚úÖ Metrics logging configured\")\n\n# ---------------------------------------------------------------------------\n# STEP 3: Create Checkpointing Options\n# ---------------------------------------------------------------------------\nprint(\"\\nüìå Step 3: Creating checkpoint manager...\")\n\ncheckpointing_options = ocp.CheckpointManagerOptions(\n    max_to_keep=3,\n    save_interval_steps=cfg.EVAL_EVERY_N_STEPS,\n)\nprint(\"   ‚úÖ Checkpointing configured\")\n\n# ---------------------------------------------------------------------------\n# STEP 4: Create RLTrainingConfig (THE CORRECT CLASS!)\n# ---------------------------------------------------------------------------\nprint(\"\\nüìå Step 4: Creating RLTrainingConfig...\")\n\n# THIS IS THE KEY FIX - using rl_cluster_lib.RLTrainingConfig\n# NOT TrainingConfig or UniversalTrainingConfig\ntraining_config = rl_cluster_lib.RLTrainingConfig(\n    # Required: the actual optax optimizer\n    actor_optimizer=optimizer,\n    \n    # Training schedule\n    eval_every_n_steps=cfg.EVAL_EVERY_N_STEPS,\n    max_steps=cfg.MAX_STEPS,\n    \n    # Batch sizes (keep small for memory safety)\n    mini_batch_size=cfg.MINI_BATCH_SIZE,\n    train_micro_batch_size=cfg.TRAIN_MICRO_BATCH_SIZE,\n    \n    # gradient_accumulation_steps is OPTIONAL - set to None if not needed\n    gradient_accumulation_steps=None,\n    \n    # Metrics and checkpointing\n    metrics_logging_options=metrics_logging_options,\n    checkpoint_root_directory=cfg.CKPT_DIR,\n    checkpointing_options=checkpointing_options,\n)\nprint(\"   ‚úÖ RLTrainingConfig created\")\n\n# ---------------------------------------------------------------------------\n# STEP 5: Create RolloutConfig\n# ---------------------------------------------------------------------------\nprint(\"\\nüìå Step 5: Creating RolloutConfig...\")\n\nrollout_config = base_rollout.RolloutConfig(\n    max_tokens_to_generate=cfg.MAX_GENERATION_LENGTH,\n    max_prompt_length=cfg.MAX_PROMPT_LENGTH,\n    kv_cache_size=cfg.MAX_PROMPT_LENGTH + cfg.MAX_GENERATION_LENGTH + 256,\n    temperature=cfg.TEMPERATURE,\n    top_p=cfg.TOP_P,\n    top_k=cfg.TOP_K,\n    eos_tokens=EOS_TOKENS,\n)\nprint(f\"   ‚úÖ RolloutConfig created (max_gen={cfg.MAX_GENERATION_LENGTH})\")\n\n# ---------------------------------------------------------------------------\n# STEP 6: Create ClusterConfig (NEW API with role_to_mesh)\n# ---------------------------------------------------------------------------\nprint(\"\\nüìå Step 6: Creating ClusterConfig...\")\n\n# THIS IS THE KEY FIX - using role_to_mesh dictionary\n# NOT the old inference_mesh parameter\ncluster_config = rl_cluster_lib.ClusterConfig(\n    role_to_mesh={\n        rl_cluster_lib.Role.ACTOR: mesh,\n        rl_cluster_lib.Role.REFERENCE: mesh,\n        rl_cluster_lib.Role.ROLLOUT: mesh,\n        # Note: No CRITIC for GRPO (critic-free algorithm)\n    },\n    rollout_engine='vanilla',\n    offload_to_cpu=False,  # Set True if still OOM\n    training_config=training_config,\n    rollout_config=rollout_config,\n)\nprint(\"   ‚úÖ ClusterConfig created with role_to_mesh\")\n\n# ---------------------------------------------------------------------------\n# STEP 7: Create GRPOConfig\n# ---------------------------------------------------------------------------\nprint(\"\\nüìå Step 7: Creating GRPOConfig...\")\n\ngrpo_config = GRPOConfig(\n    num_generations=cfg.NUM_GENERATIONS,\n    num_iterations=cfg.NUM_ITERATIONS,\n    beta=cfg.BETA,\n    epsilon=cfg.EPSILON,\n)\nprint(f\"   ‚úÖ GRPOConfig created (G={cfg.NUM_GENERATIONS}, Œ≤={cfg.BETA})\")\n\n# ---------------------------------------------------------------------------\n# STEP 8: Create RLCluster\n# ---------------------------------------------------------------------------\nprint(\"\\nüìå Step 8: Creating RLCluster...\")\n\nrl_cluster = rl_cluster_lib.RLCluster(\n    actor=actor,\n    reference=reference,\n    tokenizer=tokenizer,\n    cluster_config=cluster_config,\n)\nprint(\"   ‚úÖ RLCluster created\")\n\n# ---------------------------------------------------------------------------\n# STEP 9: Create GRPOLearner\n# ---------------------------------------------------------------------------\nprint(\"\\nüìå Step 9: Creating GRPOLearner...\")\n\n# Note: parameter is \"algo_config\", NOT \"grpo_config\"\ngrpo_trainer = GRPOLearner(\n    rl_cluster=rl_cluster,\n    reward_fns=[\n        match_format_exactly,\n        match_format_approximately,\n        check_answer,\n        check_numbers,\n    ],\n    algo_config=grpo_config,\n)\nprint(\"   ‚úÖ GRPOLearner created\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"üéâ GRPO TRAINER SETUP COMPLETE!\")\nprint(\"=\"*60)\n\nmonitor.print_summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 11: Prepare Training Data","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# PREPARE TRAINING DATA\n# =============================================================================\nimport grain\n\nprint(\"=\"*60)\nprint(\"üìä PREPARING TRAINING DATA\")\nprint(\"=\"*60)\n\n# Create training dataset in the format tunix expects\n# Each element should be a dict with 'prompts' key\n\nclass GSM8KDataset:\n    def __init__(self, prompts, answers):\n        self.prompts = prompts\n        self.answers = answers\n    \n    def __len__(self):\n        return len(self.prompts)\n    \n    def __iter__(self):\n        for prompt, answer in zip(self.prompts, self.answers):\n            yield {\n                'prompts': prompt,\n                'expected_answers': answer,  # For reward function\n            }\n\ntrain_dataset = GSM8KDataset(train_prompts, train_answers)\nval_dataset = GSM8KDataset(test_prompts, test_answers)\n\nprint(f\"\\n‚úÖ Training dataset: {len(train_dataset)} samples\")\nprint(f\"‚úÖ Validation dataset: {len(val_dataset)} samples\")\n\n# Calculate effective batches\neffective_batch = cfg.TRAIN_MICRO_BATCH_SIZE * cfg.NUM_GENERATIONS\nnum_batches = len(train_dataset) // effective_batch\n\nprint(f\"\\nEffective batch size: {effective_batch}\")\nprint(f\"Number of batches per epoch: {num_batches}\")\nprint(f\"Total training steps: {cfg.MAX_STEPS}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 12: Run Training","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# RUN TRAINING\n# =============================================================================\nimport time\n\nprint(\"=\"*60)\nprint(\"üèÉ STARTING GRPO TRAINING\")\nprint(\"=\"*60)\n\nprint(f\"\\nConfiguration:\")\nprint(f\"  ‚Ä¢ Max steps: {cfg.MAX_STEPS}\")\nprint(f\"  ‚Ä¢ Eval every: {cfg.EVAL_EVERY_N_STEPS} steps\")\nprint(f\"  ‚Ä¢ Generations per prompt (G): {cfg.NUM_GENERATIONS}\")\nprint(f\"  ‚Ä¢ Iterations per batch (Œº): {cfg.NUM_ITERATIONS}\")\nprint(f\"  ‚Ä¢ Batch size: {cfg.TRAIN_MICRO_BATCH_SIZE}\")\n\nprint(f\"\\nReward weights:\")\nprint(f\"  ‚Ä¢ Format: {cfg.REWARD_WEIGHT_FORMAT*100}%\")\nprint(f\"  ‚Ä¢ Logic: {cfg.REWARD_WEIGHT_LOGIC*100}%\")\nprint(f\"  ‚Ä¢ Accuracy: {cfg.REWARD_WEIGHT_ACCURACY*100}%\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"‚è≥ First steps will take 10-15 minutes for JIT compilation...\")\nprint(\"   Subsequent steps will be much faster.\")\nprint(\"=\"*60 + \"\\n\")\n\nmonitor.print_summary()\n\nstart_time = time.time()\n\ntry:\n    with mesh:\n        grpo_trainer.train(train_dataset, val_dataset)\n    \n    elapsed = time.time() - start_time\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"üéâ TRAINING COMPLETE!\")\n    print(\"=\"*60)\n    print(f\"   Total time: {elapsed/60:.1f} minutes\")\n    print(f\"   Steps completed: {cfg.MAX_STEPS}\")\n    \nexcept KeyboardInterrupt:\n    elapsed = time.time() - start_time\n    print(f\"\\n‚ö†Ô∏è Training interrupted after {elapsed/60:.1f} minutes\")\n    \nexcept Exception as e:\n    elapsed = time.time() - start_time\n    print(f\"\\n‚ùå Training failed after {elapsed/60:.1f} minutes\")\n    print(f\"   Error: {type(e).__name__}: {e}\")\n    raise\n\nmonitor.print_summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 13: Save Model","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# SAVE MODEL\n# =============================================================================\nfrom orbax import checkpoint as ocp\n\nprint(\"=\"*60)\nprint(\"üíæ SAVING MODEL\")\nprint(\"=\"*60)\n\nsave_path = os.path.join(cfg.OUTPUT_DIR, \"actor_final\")\nos.makedirs(save_path, exist_ok=True)\n\nwith mesh:\n    # Get actor state\n    _, actor_state = nnx.split(actor)\n    \n    # Save with Orbax\n    checkpointer = ocp.StandardCheckpointer()\n    checkpointer.save(save_path, actor_state)\n\nprint(f\"\\n‚úÖ Model saved to: {save_path}\")\nprint(f\"\\nFiles in output directory:\")\nfor f in os.listdir(cfg.OUTPUT_DIR):\n    print(f\"   {f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 14: Test Inference","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# TEST INFERENCE\n# =============================================================================\nfrom tunix.generate import sampler as sampler_lib\n\nprint(\"=\"*60)\nprint(\"üß™ TESTING TRAINED MODEL\")\nprint(\"=\"*60)\n\ntest_questions = [\n    (\"If a store sells 150 apples at $4 each, what is the total revenue?\", \"600\"),\n    (\"A train travels 120 miles in 2 hours. What is its speed in miles per hour?\", \"60\"),\n    (\"Sarah has 24 cookies. She gives 1/3 to her brother and 1/4 to her sister. How many cookies does she have left?\", \"10\"),\n]\n\nwith mesh:\n    sampler = sampler_lib.Sampler(\n        model=actor,\n        tokenizer=tokenizer,\n        max_tokens=cfg.MAX_GENERATION_LENGTH,\n    )\n    \n    for i, (question, expected) in enumerate(test_questions, 1):\n        prompt = format_prompt(question)\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"Question {i}: {question}\")\n        print(f\"Expected answer: {expected}\")\n        print(\"=\"*60)\n        \n        output = sampler.generate(prompt)\n        print(f\"\\nModel output:\")\n        print(output[:1000])\n        \n        # Extract and check answer\n        match = re.search(r'<answer>(.*?)</answer>', output, re.DOTALL)\n        if match:\n            predicted = match.group(1).strip()\n            status = \"‚úÖ\" if predicted == expected else \"‚ùå\"\n            print(f\"\\nPredicted: {predicted} {status}\")\n        else:\n            print(\"\\n‚ùå No answer tags found\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üìã Cell 15: Final Summary","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# FINAL SUMMARY\n# =============================================================================\n\nprint(\"=\"*60)\nprint(\"üèÜ GRPO TRAINING COMPLETE - SUMMARY\")\nprint(\"=\"*60)\n\nprint(f\"\"\"\nArchitecture:\n  ‚Ä¢ Hardware: TPU v5e-8 ({MESH_SHAPE[0]}√ó{MESH_SHAPE[1]} mesh)\n  ‚Ä¢ Model: {cfg.MODEL_HF_NAME}\n  ‚Ä¢ Fine-Tuning: LoRA (rank={cfg.LORA_RANK}, alpha={cfg.LORA_ALPHA})\n\nGRPO Configuration:\n  ‚Ä¢ Generations (G): {cfg.NUM_GENERATIONS}\n  ‚Ä¢ Iterations (Œº): {cfg.NUM_ITERATIONS}\n  ‚Ä¢ KL Penalty (Œ≤): {cfg.BETA}\n  ‚Ä¢ Clipping (Œµ): {cfg.EPSILON}\n\nReward Weights:\n  ‚Ä¢ Format: {cfg.REWARD_WEIGHT_FORMAT*100}%\n  ‚Ä¢ Logic: {cfg.REWARD_WEIGHT_LOGIC*100}%\n  ‚Ä¢ Accuracy: {cfg.REWARD_WEIGHT_ACCURACY*100}%\n\nTraining:\n  ‚Ä¢ Steps: {cfg.MAX_STEPS}\n  ‚Ä¢ Batch size: {cfg.TRAIN_MICRO_BATCH_SIZE}\n  ‚Ä¢ Learning rate: {cfg.LEARNING_RATE}\n\nOutput:\n  ‚Ä¢ Checkpoint: {cfg.CKPT_DIR}\n  ‚Ä¢ Final model: {cfg.OUTPUT_DIR}\n\"\"\")\n\nmonitor.print_summary()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"‚úÖ Done! Your model is ready.\")\nprint(\"=\"*60)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}