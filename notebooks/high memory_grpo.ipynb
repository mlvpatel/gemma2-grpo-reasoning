{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Gemma2 Reasoning GRPO - Full Production\n",
    "\n",
    "**Google Tunix Hackathon 2026**\n",
    "\n",
    "### Core Architecture:\n",
    "- **Hardware:** TPU v5e-8 (2 Data x 4 Tensor Mesh)\n",
    "- **Model:** Gemma 2 2B-IT (CPU-Offloaded bf16 Init)\n",
    "- **Fine-Tuning:** LoRA on Attention + MLP layers\n",
    "- **Strategy:** GRPO with 16 Parallel Generations per prompt\n",
    "- **Rewards:** Format (25%) + Logic (30%) + Accuracy (45%) + Self-Correction & Length Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Cell 1: Environment Setup\n",
    "*Run once, then restart kernel*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "SETUP_MARKER = \"/kaggle/working/.setup_complete_prod_v1\"\n",
    "\n",
    "if os.path.exists(SETUP_MARKER):\n",
    "    print(\"‚úÖ Packages ready. Continuing...\")\n",
    "else:\n",
    "    print(\"=\"*60)\n",
    "    print(\"SETTING UP PRODUCTION ENVIRONMENT...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Use %pip for notebooks (recommended by Tunix docs)\n",
    "    %pip install --upgrade pip -q\n",
    "    %pip uninstall -y jax jaxlib flax optax -q 2>/dev/null\n",
    "    \n",
    "    print(\"Installing JAX/TPU stack...\")\n",
    "    %pip install -q \"jax[tpu]>=0.8.0\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
    "    %pip install -q \"numpy==2.0.0\" \"pyarrow==17.0.0\"\n",
    "    \n",
    "    print(\"Installing Tunix & Qwix from GitHub (latest)...\")\n",
    "    %pip install -q git+https://github.com/google/tunix\n",
    "    %pip install -q git+https://github.com/google/qwix\n",
    "    %pip uninstall -q flax -y 2>/dev/null\n",
    "    %pip install -q git+https://github.com/google/flax\n",
    "    \n",
    "    print(\"Installing supporting packages...\")\n",
    "    %pip install -q kagglehub transformers grain huggingface_hub tensorflow tensorflow_datasets datasets\n",
    "    \n",
    "    with open(SETUP_MARKER, \"w\") as f: \n",
    "        f.write(\"done\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ SETUP COMPLETE. RESTART KERNEL NOW!\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîå Cell 2: Imports & Memory Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, gc, time, shutil\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "import grain\n",
    "import qwix\n",
    "import kagglehub\n",
    "\n",
    "from flax import nnx\n",
    "from orbax import checkpoint as ocp\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Tunix imports\n",
    "try:\n",
    "    from tunix.models.gemma2 import model as gemma_model_lib\n",
    "    from tunix.models.gemma2 import params as gemma_params_lib\n",
    "    print(\"‚úì Using tunix.models.gemma2\")\n",
    "except ImportError:\n",
    "    from tunix.models.gemma import model as gemma_model_lib\n",
    "    from tunix.models.gemma import params as gemma_params_lib\n",
    "    print(\"‚úì Using tunix.models.gemma (fallback)\")\n",
    "\n",
    "from tunix.rl import rl_cluster as rl_cluster_lib\n",
    "from tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\n",
    "from tunix.rl.rollout import base_rollout\n",
    "from tunix.generate import sampler as sampler_lib\n",
    "\n",
    "class MemoryMonitor:\n",
    "    @staticmethod\n",
    "    def get_usage():\n",
    "        try:\n",
    "            stats = [d.memory_stats() for d in jax.devices() if d.memory_stats()]\n",
    "            if stats:\n",
    "                used = sum(s['bytes_in_use'] for s in stats)\n",
    "                limit = sum(s['bytes_limit'] for s in stats)\n",
    "                return used, limit\n",
    "        except:\n",
    "            pass\n",
    "        return 0, 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def print_summary():\n",
    "        used, limit = MemoryMonitor.get_usage()\n",
    "        if limit > 0:\n",
    "            print(f\"  TPU Memory: {used/1e9:.2f}GB / {limit/1e9:.2f}GB ({100*used/limit:.1f}%)\")\n",
    "        else:\n",
    "            print(\"  TPU Memory: stats unavailable\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_available(required_gb=20):\n",
    "        used, limit = MemoryMonitor.get_usage()\n",
    "        available = (limit - used) / 1e9\n",
    "        return available >= required_gb\n",
    "\n",
    "monitor = MemoryMonitor()\n",
    "print(f\"JAX: {jax.__version__} | TPU Cores: {len(jax.devices())}\")\n",
    "monitor.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Cell 3: Production Configuration\n",
    "*16 Generations | 600 Steps | Optimized for TPU v5e-8*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL CONFIGURATION\n",
    "# =============================================================================\n",
    "MODEL_VERSION = \"gemma2-2b-it\"\n",
    "MODEL_PATH = \"google/gemma-2/flax/gemma2-2b-it\"\n",
    "MODEL_HF_NAME = \"google/gemma-2-2b-it\"\n",
    "\n",
    "# =============================================================================\n",
    "# MESH CONFIGURATION\n",
    "# TPU v5e-8: 8 cores, (2,4) mesh = 2 FSDP √ó 4 TP\n",
    "# tp=4 splits the 4 KV-heads across 4 cores (1 head per core)\n",
    "# fsdp=2 splits data/weights across 2 groups\n",
    "# =============================================================================\n",
    "MESH_SHAPE = (2, 4)\n",
    "MESH_AXES = (\"fsdp\", \"tp\")\n",
    "\n",
    "# =============================================================================\n",
    "# LORA CONFIGURATION\n",
    "# Targeting Attention (q, k, v, o) + MLP (gate, up, down) layers\n",
    "# =============================================================================\n",
    "LORA_RANK = 64\n",
    "LORA_ALPHA = 64.0\n",
    "# Regex pattern for Gemma2 layer names\n",
    "LORA_TARGET_PATTERN = \".*q_einsum|.*kv_einsum|.*o_proj|.*gate_proj|.*up_proj|.*down_proj\"\n",
    "\n",
    "# =============================================================================\n",
    "# GRPO CONFIGURATION (Production Settings)\n",
    "# G=16: Generate 16 responses per prompt for statistical power\n",
    "# Œº=3: 3 iterations per batch for stable updates\n",
    "# Œ≤=0.04: KL penalty coefficient\n",
    "# Œµ=0.2: PPO-style clipping\n",
    "# =============================================================================\n",
    "NUM_GENERATIONS = 16    # G in GRPO paper - 16 parallel generations\n",
    "NUM_ITERATIONS = 3      # Œº in GRPO paper - iterations per batch\n",
    "BETA = 0.04             # KL divergence penalty\n",
    "EPSILON = 0.2           # Clipping parameter\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING CONFIGURATION\n",
    "# =============================================================================\n",
    "MAX_STEPS = 600\n",
    "LEARNING_RATE = 2e-6\n",
    "WARMUP_STEPS = 40\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "# Batch sizes - carefully tuned for 16 generations\n",
    "# full_batch = num_samples √ó num_generations must be divisible by mini_batch\n",
    "MINI_BATCH_SIZE = 8\n",
    "MICRO_BATCH_SIZE = 1    # Gradient accumulation for memory efficiency\n",
    "\n",
    "# =============================================================================\n",
    "# SEQUENCE CONFIGURATION\n",
    "# =============================================================================\n",
    "MAX_PROMPT_LENGTH = 256\n",
    "MAX_GENERATION_LENGTH = 768  # Allow long reasoning chains\n",
    "\n",
    "# =============================================================================\n",
    "# REWARD WEIGHTS (Total = 100%)\n",
    "# =============================================================================\n",
    "REWARD_WEIGHT_FORMAT = 0.25      # 25% - Correct XML structure\n",
    "REWARD_WEIGHT_LOGIC = 0.30       # 30% - Quality reasoning with transitions\n",
    "REWARD_WEIGHT_ACCURACY = 0.45    # 45% - Correct final answer\n",
    "# Self-correction and length are additive bonuses (not weighted)\n",
    "\n",
    "# =============================================================================\n",
    "# OUTPUT TAGS\n",
    "# =============================================================================\n",
    "TAG_REASONING_START = \"<reasoning>\"\n",
    "TAG_REASONING_END = \"</reasoning>\"\n",
    "TAG_ANSWER_START = \"<answer>\"\n",
    "TAG_ANSWER_END = \"</answer>\"\n",
    "\n",
    "# =============================================================================\n",
    "# PATHS\n",
    "# =============================================================================\n",
    "CKPT_DIR = \"/kaggle/working/checkpoints\"\n",
    "OUTPUT_DIR = \"/kaggle/working/gemma2-grpo-16gen\"\n",
    "\n",
    "# =============================================================================\n",
    "# DATA CONFIGURATION\n",
    "# num_samples must create full_batch divisible by MINI_BATCH_SIZE\n",
    "# With G=16, if we use 32 samples: 32√ó16=512, 512/8=64 ‚úì\n",
    "# =============================================================================\n",
    "NUM_TRAIN_SAMPLES = 448  # 448 √ó 16 = 7168, divisible by 8\n",
    "NUM_TEST_SAMPLES = 64    # 64 √ó 16 = 1024, divisible by 8\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PRODUCTION CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: {MODEL_HF_NAME}\")\n",
    "print(f\"Mesh: {MESH_SHAPE} ({MESH_AXES})\")\n",
    "print(f\"LoRA: rank={LORA_RANK}, alpha={LORA_ALPHA}\")\n",
    "print(f\"GRPO: G={NUM_GENERATIONS}, Œº={NUM_ITERATIONS}, Œ≤={BETA}, Œµ={EPSILON}\")\n",
    "print(f\"Training: {MAX_STEPS} steps, LR={LEARNING_RATE}\")\n",
    "print(f\"Batch: mini={MINI_BATCH_SIZE}, micro={MICRO_BATCH_SIZE}\")\n",
    "print(f\"Data: {NUM_TRAIN_SAMPLES} train, {NUM_TEST_SAMPLES} test\")\n",
    "print(f\"Rewards: Format={REWARD_WEIGHT_FORMAT*100}%, Logic={REWARD_WEIGHT_LOGIC*100}%, Accuracy={REWARD_WEIGHT_ACCURACY*100}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Cell 4: Weighted Reward Functions\n",
    "*Format (25%) + Logic (30%) + Accuracy (45%) + Bonuses*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_reward(prompts, completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward for correct XML tag structure (25% weight).\n",
    "    Checks: <reasoning>...</reasoning> followed by <answer>...</answer>\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for c in completions:\n",
    "        score = 0.0\n",
    "        has_r_start = TAG_REASONING_START in c\n",
    "        has_r_end = TAG_REASONING_END in c\n",
    "        has_a_start = TAG_ANSWER_START in c\n",
    "        has_a_end = TAG_ANSWER_END in c\n",
    "        \n",
    "        # Full format with correct ordering\n",
    "        if has_r_start and has_r_end and has_a_start and has_a_end:\n",
    "            r_end_pos = c.find(TAG_REASONING_END)\n",
    "            a_start_pos = c.find(TAG_ANSWER_START)\n",
    "            if r_end_pos < a_start_pos:\n",
    "                score = 1.0  # Perfect format\n",
    "            else:\n",
    "                score = 0.5  # Wrong order\n",
    "        # Partial format\n",
    "        elif (has_r_start and has_r_end) or (has_a_start and has_a_end):\n",
    "            score = 0.3\n",
    "        \n",
    "        rewards.append(score * REWARD_WEIGHT_FORMAT)\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def logic_reward(prompts, completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward for quality reasoning with logical transitions (30% weight).\n",
    "    Checks: step-by-step reasoning, transition words, mathematical operations.\n",
    "    \"\"\"\n",
    "    TRANSITIONS = [\n",
    "        'therefore', 'because', 'since', 'so', 'thus', 'hence',\n",
    "        'first', 'second', 'third', 'then', 'next', 'finally',\n",
    "        'step', 'calculate', 'compute', 'multiply', 'divide', 'add', 'subtract'\n",
    "    ]\n",
    "    \n",
    "    rewards = []\n",
    "    for c in completions:\n",
    "        score = 0.0\n",
    "        \n",
    "        # Extract reasoning section\n",
    "        match = re.search(f'{TAG_REASONING_START}(.*?){TAG_REASONING_END}', c, re.DOTALL | re.IGNORECASE)\n",
    "        if match:\n",
    "            content = match.group(1).lower()\n",
    "            \n",
    "            # Count transition words (up to 0.4)\n",
    "            trans_count = sum(1 for t in TRANSITIONS if t in content)\n",
    "            score += min(0.4, trans_count * 0.05)\n",
    "            \n",
    "            # Count explicit steps like \"Step 1:\", \"Step 2:\" (up to 0.3)\n",
    "            step_count = len(re.findall(r'step\\s*\\d', content))\n",
    "            score += min(0.3, step_count * 0.1)\n",
    "            \n",
    "            # Check for mathematical expressions (up to 0.2)\n",
    "            math_ops = len(re.findall(r'\\d+\\s*[+\\-√ó√∑*/]\\s*\\d+', content))\n",
    "            score += min(0.2, math_ops * 0.05)\n",
    "            \n",
    "            # Bonus for \"equals\" or \"=\" showing computation (up to 0.1)\n",
    "            equals_count = content.count('=') + content.count('equals')\n",
    "            score += min(0.1, equals_count * 0.02)\n",
    "        \n",
    "        rewards.append(min(1.0, score) * REWARD_WEIGHT_LOGIC)\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def accuracy_reward(prompts, completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward for correct numerical answer (45% weight).\n",
    "    Compares extracted answer with ground truth.\n",
    "    \"\"\"\n",
    "    answers = kwargs.get('answers', [])\n",
    "    rewards = []\n",
    "    \n",
    "    for i, c in enumerate(completions):\n",
    "        score = 0.0\n",
    "        \n",
    "        if i >= len(answers):\n",
    "            rewards.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        # Extract answer from completion\n",
    "        match = re.search(f'{TAG_ANSWER_START}(.*?){TAG_ANSWER_END}', c, re.DOTALL)\n",
    "        if not match:\n",
    "            rewards.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        pred_text = match.group(1).strip()\n",
    "        truth_text = str(answers[i]).strip()\n",
    "        \n",
    "        # Extract numbers\n",
    "        pred_nums = re.findall(r'-?\\d+\\.?\\d*', pred_text)\n",
    "        truth_nums = re.findall(r'-?\\d+\\.?\\d*', truth_text)\n",
    "        \n",
    "        if pred_nums and truth_nums:\n",
    "            try:\n",
    "                pred = float(pred_nums[-1])  # Use last number as final answer\n",
    "                truth = float(truth_nums[-1])\n",
    "                \n",
    "                if pred == truth:\n",
    "                    score = 1.0  # Exact match\n",
    "                elif abs(truth) > 0.001:\n",
    "                    ratio = pred / truth\n",
    "                    if 0.99 <= ratio <= 1.01:\n",
    "                        score = 0.9  # Within 1%\n",
    "                    elif 0.9 <= ratio <= 1.1:\n",
    "                        score = 0.5  # Within 10%\n",
    "            except (ValueError, ZeroDivisionError):\n",
    "                pass\n",
    "        \n",
    "        rewards.append(score * REWARD_WEIGHT_ACCURACY)\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def self_correction_reward(prompts, completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Bonus reward for self-correction behavior (additive, max 0.1).\n",
    "    Encourages the model to catch and fix its own mistakes.\n",
    "    \"\"\"\n",
    "    CORRECTION_PHRASES = [\n",
    "        \"wait\", \"actually\", \"let me recalculate\", \"i made an error\",\n",
    "        \"correction\", \"re-checking\", \"that's not right\", \"let me redo\"\n",
    "    ]\n",
    "    \n",
    "    rewards = []\n",
    "    for c in completions:\n",
    "        c_lower = c.lower()\n",
    "        count = sum(1 for phrase in CORRECTION_PHRASES if phrase in c_lower)\n",
    "        # Cap at 0.1 bonus\n",
    "        rewards.append(min(0.1, count * 0.03))\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def length_reward(prompts, completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Length regularization (additive, max 0.05).\n",
    "    Rewards appropriate response length, penalizes too short/long.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for c in completions:\n",
    "        words = len(c.split())\n",
    "        if 100 <= words <= 400:\n",
    "            rewards.append(0.05)  # Ideal length\n",
    "        elif 50 <= words <= 600:\n",
    "            rewards.append(0.02)  # Acceptable\n",
    "        else:\n",
    "            rewards.append(0.0)   # Too short or too long\n",
    "    return rewards\n",
    "\n",
    "\n",
    "# Combine all reward functions\n",
    "REWARD_FUNCTIONS = [\n",
    "    format_reward,      # 25% weight\n",
    "    logic_reward,       # 30% weight  \n",
    "    accuracy_reward,    # 45% weight\n",
    "    self_correction_reward,  # Bonus up to 10%\n",
    "    length_reward,      # Bonus up to 5%\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Reward Functions Defined\")\n",
    "print(f\"   ‚Ä¢ format_reward: {REWARD_WEIGHT_FORMAT*100}% weight\")\n",
    "print(f\"   ‚Ä¢ logic_reward: {REWARD_WEIGHT_LOGIC*100}% weight\")\n",
    "print(f\"   ‚Ä¢ accuracy_reward: {REWARD_WEIGHT_ACCURACY*100}% weight\")\n",
    "print(f\"   ‚Ä¢ self_correction_reward: bonus (max 10%)\")\n",
    "print(f\"   ‚Ä¢ length_reward: bonus (max 5%)\")\n",
    "print(f\"   Total potential: {(REWARD_WEIGHT_FORMAT + REWARD_WEIGHT_LOGIC + REWARD_WEIGHT_ACCURACY)*100}% + 15% bonus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Cell 5: Data Loading (GSM8K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt teaching the expected output format\n",
    "SYSTEM_PROMPT = f\"\"\"You are a mathematical problem solver who shows their work.\n",
    "\n",
    "For each problem:\n",
    "1. Think through it step-by-step inside {TAG_REASONING_START} and {TAG_REASONING_END} tags\n",
    "2. Show your calculations clearly\n",
    "3. Give your final numerical answer inside {TAG_ANSWER_START} and {TAG_ANSWER_END} tags\n",
    "\n",
    "Example:\n",
    "{TAG_REASONING_START}\n",
    "Step 1: Identify what we need to find.\n",
    "Step 2: Set up the calculation.\n",
    "Step 3: 5 √ó 10 = 50\n",
    "Therefore, the answer is 50.\n",
    "{TAG_REASONING_END}\n",
    "{TAG_ANSWER_START}\n",
    "50\n",
    "{TAG_ANSWER_END}\"\"\"\n",
    "\n",
    "def format_prompt(question: str) -> str:\n",
    "    \"\"\"Format a question with the system prompt.\"\"\"\n",
    "    return f\"{SYSTEM_PROMPT}\\n\\nQuestion: {question}\\nSolution:\"\n",
    "\n",
    "def extract_answer(answer_text: str) -> str:\n",
    "    \"\"\"Extract the final answer from GSM8K format (after ####).\"\"\"\n",
    "    if '####' in answer_text:\n",
    "        return answer_text.split('####')[-1].strip()\n",
    "    return answer_text.strip()\n",
    "\n",
    "# Load GSM8K dataset\n",
    "print(\"Loading GSM8K dataset...\")\n",
    "dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\")\n",
    "dataset = dataset.shuffle(seed=RANDOM_SEED)\n",
    "\n",
    "# Prepare data\n",
    "all_data = []\n",
    "for i, item in enumerate(dataset):\n",
    "    if i >= NUM_TRAIN_SAMPLES + NUM_TEST_SAMPLES:\n",
    "        break\n",
    "    all_data.append({\n",
    "        'prompt': format_prompt(item['question']),\n",
    "        'answer': extract_answer(item['answer'])\n",
    "    })\n",
    "\n",
    "train_data = all_data[:NUM_TRAIN_SAMPLES]\n",
    "test_data = all_data[NUM_TRAIN_SAMPLES:NUM_TRAIN_SAMPLES + NUM_TEST_SAMPLES]\n",
    "\n",
    "# Verify batch alignment\n",
    "full_batch_train = len(train_data) * NUM_GENERATIONS\n",
    "full_batch_test = len(test_data) * NUM_GENERATIONS\n",
    "\n",
    "print(f\"‚úÖ Data Loaded\")\n",
    "print(f\"   Train: {len(train_data)} samples √ó {NUM_GENERATIONS} gen = {full_batch_train} (√∑{MINI_BATCH_SIZE}={full_batch_train//MINI_BATCH_SIZE})\")\n",
    "print(f\"   Test: {len(test_data)} samples √ó {NUM_GENERATIONS} gen = {full_batch_test} (√∑{MINI_BATCH_SIZE}={full_batch_test//MINI_BATCH_SIZE})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Cell 6: Mesh & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mesh for distributed training\n",
    "mesh = jax.make_mesh(\n",
    "    MESH_SHAPE, \n",
    "    MESH_AXES, \n",
    "    axis_types=(jax.sharding.AxisType.Auto,) * len(MESH_SHAPE)\n",
    ")\n",
    "print(f\"‚úì Mesh created: {mesh}\")\n",
    "\n",
    "# Get HuggingFace token\n",
    "hf_token = os.environ.get('HF_TOKEN')\n",
    "if not hf_token:\n",
    "    try:\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        hf_token = UserSecretsClient().get_secret(\"HF_TOKEN\")\n",
    "        print(\"‚úì HF Token from Kaggle Secrets\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è No HF_TOKEN found: {e}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_HF_NAME, token=hf_token)\n",
    "\n",
    "# Get EOS tokens for generation stopping\n",
    "EOS_TOKENS = [tokenizer.eos_token_id]\n",
    "try:\n",
    "    end_turn_id = tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")\n",
    "    if end_turn_id != tokenizer.unk_token_id:\n",
    "        EOS_TOKENS.append(end_turn_id)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(f\"‚úÖ Tokenizer Ready | EOS tokens: {EOS_TOKENS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Cell 7: CPU-Offload Model Loading + LoRA\n",
    "*Build on CPU first to prevent TPU OOM during initialization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 1: Clean memory\n",
    "# =============================================================================\n",
    "gc.collect()\n",
    "jax.clear_caches()\n",
    "print(\"1. Memory cleared\")\n",
    "monitor.print_summary()\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: Download model\n",
    "# =============================================================================\n",
    "print(\"\\n2. Downloading model from Kaggle...\")\n",
    "k_path = kagglehub.model_download(MODEL_PATH)\n",
    "ckpt_path = os.path.join(k_path, MODEL_VERSION)\n",
    "print(f\"   Path: {ckpt_path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: Get CPU device for offloading\n",
    "# =============================================================================\n",
    "try:\n",
    "    cpu_device = jax.devices('cpu')[0]\n",
    "    print(f\"\\n3. CPU device for offloading: {cpu_device}\")\n",
    "except:\n",
    "    cpu_device = None\n",
    "    print(\"\\n3. No separate CPU device, using default\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: Build model blueprint on CPU (prevents TPU init spike)\n",
    "# =============================================================================\n",
    "print(\"\\n4. Building model on CPU (bf16)...\")\n",
    "config = gemma_model_lib.ModelConfig.gemma2_2b_it()\n",
    "\n",
    "# Build model structure\n",
    "base_model = gemma_model_lib.Gemma(config, rngs=nnx.Rngs(RANDOM_SEED))\n",
    "\n",
    "# Load and convert weights to bf16\n",
    "print(\"   Loading checkpoint...\")\n",
    "raw_params = gemma_params_lib.load_and_format_params(ckpt_path)\n",
    "print(\"   Converting to bf16...\")\n",
    "bf16_params = jax.tree.map(lambda x: x.astype(jnp.bfloat16), raw_params)\n",
    "nnx.update(base_model, bf16_params)\n",
    "\n",
    "# Clean up\n",
    "del raw_params, bf16_params\n",
    "gc.collect()\n",
    "print(\"   ‚úì Model built on CPU\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 5: Shard to TPU mesh\n",
    "# =============================================================================\n",
    "print(\"\\n5. Sharding to TPU mesh (2√ó4)...\")\n",
    "with mesh:\n",
    "    graph, state = nnx.split(base_model)\n",
    "    # Create reference model (frozen, for KL penalty)\n",
    "    ref_model = nnx.merge(graph, state)\n",
    "    # Create actor model (will have LoRA applied)\n",
    "    actor = nnx.merge(graph, state)\n",
    "\n",
    "del base_model\n",
    "gc.collect()\n",
    "print(\"   ‚úì Models sharded to TPU\")\n",
    "monitor.print_summary()\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 6: Apply LoRA to actor model\n",
    "# =============================================================================\n",
    "print(\"\\n6. Applying LoRA (rank=64)...\")\n",
    "print(f\"   Target pattern: {LORA_TARGET_PATTERN}\")\n",
    "\n",
    "# Create LoRA provider\n",
    "lora_provider = qwix.LoraProvider(\n",
    "    module_path=LORA_TARGET_PATTERN,\n",
    "    rank=LORA_RANK,\n",
    "    alpha=LORA_ALPHA,\n",
    ")\n",
    "\n",
    "# Get model input shape\n",
    "model_input = actor.get_model_input()\n",
    "\n",
    "# Apply LoRA\n",
    "actor = qwix.apply_lora_to_model(\n",
    "    actor,\n",
    "    lora_provider,\n",
    "    rngs=nnx.Rngs(RANDOM_SEED),\n",
    "    **model_input\n",
    ")\n",
    "\n",
    "# Re-shard after LoRA application\n",
    "with mesh:\n",
    "    state = nnx.state(actor)\n",
    "    pspecs = nnx.get_partition_spec(state)\n",
    "    sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
    "    nnx.update(actor, sharded_state)\n",
    "\n",
    "print(\"   ‚úì LoRA applied to Attention + MLP layers\")\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL STATUS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ MODEL READY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"   Actor: Gemma2-2B-IT + LoRA (rank={LORA_RANK})\")\n",
    "print(f\"   Reference: Gemma2-2B-IT (frozen)\")\n",
    "monitor.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Cell 8: Data Loaders (Grain MapDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSM8KDataSource:\n",
    "    \"\"\"Data source compatible with Grain MapDataset.\"\"\"\n",
    "    def __init__(self, data):\n",
    "        self._data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self._data[idx]\n",
    "        # Tunix GRPO expects 'prompts' and 'answers' keys\n",
    "        return {\n",
    "            'prompts': item['prompt'],\n",
    "            'answers': item['answer']\n",
    "        }\n",
    "\n",
    "# Create Grain MapDataset pipelines\n",
    "# Using the same pattern as official Tunix GRPO demo\n",
    "train_dataset = (\n",
    "    grain.MapDataset.source(GSM8KDataSource(train_data))\n",
    "    .shuffle(seed=RANDOM_SEED)\n",
    ")\n",
    "\n",
    "val_dataset = (\n",
    "    grain.MapDataset.source(GSM8KDataSource(test_data))\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Grain Datasets Created\")\n",
    "print(f\"   Train: {len(train_data)} prompts\")\n",
    "print(f\"   Val: {len(test_data)} prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Cell 9: Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate schedule with warmup and cosine decay\n",
    "lr_schedule = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=0.0,\n",
    "    peak_value=LEARNING_RATE,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    decay_steps=MAX_STEPS,\n",
    "    end_value=LEARNING_RATE * 0.1\n",
    ")\n",
    "\n",
    "# Optimizer with gradient clipping\n",
    "optimizer = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),\n",
    "    optax.adamw(lr_schedule, weight_decay=WEIGHT_DECAY)\n",
    ")\n",
    "\n",
    "# Cluster configuration\n",
    "cluster_config = rl_cluster_lib.ClusterConfig(\n",
    "    role_to_mesh={\n",
    "        rl_cluster_lib.Role.ACTOR: mesh,\n",
    "        rl_cluster_lib.Role.REFERENCE: mesh,\n",
    "        rl_cluster_lib.Role.ROLLOUT: mesh,\n",
    "    },\n",
    "    training_config=rl_cluster_lib.RLTrainingConfig(\n",
    "        actor_optimizer=optimizer,\n",
    "        max_steps=MAX_STEPS,\n",
    "        eval_every_n_steps=25,\n",
    "        mini_batch_size=MINI_BATCH_SIZE,\n",
    "        train_micro_batch_size=MICRO_BATCH_SIZE,\n",
    "        checkpoint_root_directory=CKPT_DIR,\n",
    "    ),\n",
    "    rollout_config=base_rollout.RolloutConfig(\n",
    "        max_tokens_to_generate=MAX_GENERATION_LENGTH,\n",
    "        max_prompt_length=MAX_PROMPT_LENGTH,\n",
    "        temperature=0.9,\n",
    "        top_p=0.95,\n",
    "        eos_tokens=EOS_TOKENS,\n",
    "    )\n",
    ")\n",
    "\n",
    "# GRPO configuration - 16 generations!\n",
    "grpo_config = GRPOConfig(\n",
    "    num_generations=NUM_GENERATIONS,  # G=16\n",
    "    num_iterations=NUM_ITERATIONS,    # Œº=3\n",
    "    beta=BETA,                        # Œ≤=0.04\n",
    "    epsilon=EPSILON,                  # Œµ=0.2\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training Configuration\")\n",
    "print(f\"   LR Schedule: warmup {WARMUP_STEPS} ‚Üí {LEARNING_RATE} ‚Üí cosine decay\")\n",
    "print(f\"   Optimizer: AdamW (clip=1.0, decay={WEIGHT_DECAY})\")\n",
    "print(f\"   GRPO: G={NUM_GENERATIONS}, Œº={NUM_ITERATIONS}, Œ≤={BETA}, Œµ={EPSILON}\")\n",
    "print(f\"   Eval every {25} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Cell 10: Create GRPO Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "jax.clear_caches()\n",
    "\n",
    "print(\"Creating RLCluster and GRPOLearner...\")\n",
    "\n",
    "with mesh:\n",
    "    # Create RL cluster\n",
    "    rl_cluster = rl_cluster_lib.RLCluster(\n",
    "        actor=actor,\n",
    "        reference=ref_model,\n",
    "        tokenizer=tokenizer,\n",
    "        cluster_config=cluster_config,\n",
    "    )\n",
    "    \n",
    "    # Create GRPO trainer with weighted reward functions\n",
    "    # NOTE: Use 'algo_config' (not 'grpo_config') per latest Tunix API\n",
    "    grpo_trainer = GRPOLearner(\n",
    "        rl_cluster=rl_cluster,\n",
    "        reward_fns=REWARD_FUNCTIONS,\n",
    "        algo_config=grpo_config,\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ GRPO TRAINER READY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"   Generations per prompt: {NUM_GENERATIONS}\")\n",
    "print(f\"   Reward functions: {len(REWARD_FUNCTIONS)}\")\n",
    "print(f\"     - format_reward (25%)\")\n",
    "print(f\"     - logic_reward (30%)\")\n",
    "print(f\"     - accuracy_reward (45%)\")\n",
    "print(f\"     - self_correction_reward (bonus)\")\n",
    "print(f\"     - length_reward (bonus)\")\n",
    "monitor.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Cell 11: Training Loop\n",
    "*First steps take 10-15 minutes for JIT compilation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üöÄ STARTING GRPO TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\")\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  ‚Ä¢ Steps: {MAX_STEPS}\")\n",
    "print(f\"  ‚Ä¢ Generations per prompt: {NUM_GENERATIONS}\")\n",
    "print(f\"  ‚Ä¢ Iterations per batch: {NUM_ITERATIONS}\")\n",
    "print(f\"  ‚Ä¢ Train samples: {NUM_TRAIN_SAMPLES}\")\n",
    "print(f\"\")\n",
    "print(f\"Reward Weights:\")\n",
    "print(f\"  ‚Ä¢ Format: {REWARD_WEIGHT_FORMAT*100}%\")\n",
    "print(f\"  ‚Ä¢ Logic: {REWARD_WEIGHT_LOGIC*100}%\")\n",
    "print(f\"  ‚Ä¢ Accuracy: {REWARD_WEIGHT_ACCURACY*100}%\")\n",
    "print(f\"\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n‚è≥ First steps will take 10-15 minutes for JIT compilation...\")\n",
    "print(\"   Subsequent steps will be much faster.\\n\")\n",
    "\n",
    "monitor.print_summary()\n",
    "start_time = time.time()\n",
    "\n",
    "# Run training\n",
    "with mesh:\n",
    "    grpo_trainer.train(train_dataset, val_dataset)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ TRAINING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"   Total time: {elapsed/60:.1f} minutes\")\n",
    "print(f\"   Steps completed: {MAX_STEPS}\")\n",
    "monitor.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Cell 12: Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving trained model...\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "with mesh:\n",
    "    _, actor_state = nnx.split(actor)\n",
    "    checkpointer = ocp.StandardCheckpointer()\n",
    "    save_path = os.path.join(OUTPUT_DIR, \"actor_state\")\n",
    "    checkpointer.save(save_path, actor_state)\n",
    "\n",
    "print(f\"‚úÖ Model saved to {OUTPUT_DIR}\")\n",
    "print(f\"   Checkpoint: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Cell 13: Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = [\n",
    "    (\"If a store sells 150 apples at $4 each, what is the total revenue?\", \"600\"),\n",
    "    (\"A train travels 120 miles in 2 hours. What is its speed in miles per hour?\", \"60\"),\n",
    "    (\"Sarah has 24 cookies. She gives 1/3 to her brother and 1/4 to her sister. How many cookies does she have left?\", \"10\"),\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üß™ TESTING TRAINED MODEL\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "with mesh:\n",
    "    sampler = sampler_lib.Sampler(\n",
    "        model=actor,\n",
    "        tokenizer=tokenizer,\n",
    "        max_tokens=MAX_GENERATION_LENGTH\n",
    "    )\n",
    "    \n",
    "    for i, (question, expected) in enumerate(test_questions, 1):\n",
    "        prompt = format_prompt(question)\n",
    "        output = sampler.generate(prompt)\n",
    "        \n",
    "        print(f\"Question {i}: {question}\")\n",
    "        print(f\"Expected: {expected}\")\n",
    "        print(f\"Output:\")\n",
    "        print(output[:800])\n",
    "        print(\"-\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Cell 14: Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üèÜ PRODUCTION REASONING MODEL COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\")\n",
    "print(f\"Architecture:\")\n",
    "print(f\"  ‚Ä¢ Hardware: TPU v5e-8 (2√ó4 mesh)\")\n",
    "print(f\"  ‚Ä¢ Model: {MODEL_HF_NAME}\")\n",
    "print(f\"  ‚Ä¢ Fine-Tuning: LoRA (rank={LORA_RANK}, alpha={LORA_ALPHA})\")\n",
    "print(f\"  ‚Ä¢ Targets: Attention + MLP layers\")\n",
    "print(f\"\")\n",
    "print(f\"GRPO Configuration:\")\n",
    "print(f\"  ‚Ä¢ Generations (G): {NUM_GENERATIONS}\")\n",
    "print(f\"  ‚Ä¢ Iterations (Œº): {NUM_ITERATIONS}\")\n",
    "print(f\"  ‚Ä¢ KL Penalty (Œ≤): {BETA}\")\n",
    "print(f\"  ‚Ä¢ Clipping (Œµ): {EPSILON}\")\n",
    "print(f\"\")\n",
    "print(f\"Reward Weights:\")\n",
    "print(f\"  ‚Ä¢ Format: {REWARD_WEIGHT_FORMAT*100}%\")\n",
    "print(f\"  ‚Ä¢ Logic: {REWARD_WEIGHT_LOGIC*100}%\")\n",
    "print(f\"  ‚Ä¢ Accuracy: {REWARD_WEIGHT_ACCURACY*100}%\")\n",
    "print(f\"  ‚Ä¢ Self-Correction: bonus\")\n",
    "print(f\"  ‚Ä¢ Length: bonus\")\n",
    "print(f\"\")\n",
    "print(f\"Training:\")\n",
    "print(f\"  ‚Ä¢ Steps: {MAX_STEPS}\")\n",
    "print(f\"  ‚Ä¢ Samples: {NUM_TRAIN_SAMPLES} train, {NUM_TEST_SAMPLES} test\")\n",
    "print(f\"  ‚Ä¢ Output: {OUTPUT_DIR}\")\n",
    "print(f\"\")\n",
    "monitor.print_summary()\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
